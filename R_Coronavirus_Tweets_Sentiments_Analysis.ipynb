{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "GF8Ens_Soomf",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejask108/Coronavirus_Tweets_Sentiment_Analysis/blob/main/R_Coronavirus_Tweets_Sentiments_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Coronavirus_Tweets_Sentiment_Analysis**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type    - Classification**\n",
        "##### **Contribution    - Team**\n",
        "##### **Team Member 1 -Rakshanda Shaikh**\n",
        "##### **Team Member 2 -Nirvi Prabhale**\n",
        "##### **Team Member 3 -Tejesh Khadke**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CoVid-19 pandemic has had a profound impact on society, disrupting lives and livelihoods worldwide. Lockdown measures forced people to stay indoors, leading to significant changes in daily routines and interactions. During these challenging times, Twitter emerged as a prominent platform for people to express their thoughts and emotions, making it an essential source for understanding public sentiments.\n",
        "\n",
        "Our analysis focused on gauging the sentiments of individuals by analyzing their tweets on Twitter. We aimed to comprehend the prevailing emotions and opinions of people during the pandemic through text data.\n",
        "\n",
        "Overall, our analysis aimed to understand the sentiments expressed by people on Twitter during the pandemic and explore which machine learning algorithms and vectorization techniques perform best in sentiment classification. By leveraging Twitter data, we gained valuable insights into the prevailing sentiments and emotions of individuals during these unprecedented times."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Rakshanda19/Coronavirus-Tweets-Sentiments-Analysis..git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this challenge, we are tasked with building a classification model to predict the sentiment of COVID-19 related tweets. The dataset consists of tweets collected from Twitter, and manual tagging has been performed to label each tweet with its corresponding sentiment.\n",
        "\n",
        "The information provided in the dataset includes:\n",
        "\n",
        "**-Location:** The geographical location associated with the tweet, indicating where it was posted from.\n",
        "\n",
        "**-TweetAt:**The timestamp or date when the tweet was posted.\n",
        "\n",
        "**-Original Tweet:** The actual text content of the tweet.\n",
        "\n",
        "**-Sentiment:** The sentiment label assigned to each tweet, indicating whether it is positive, negative, or neutral in tone.\n",
        "\n",
        "**-User Name:** A code or identifier for the Twitter user who posted the tweet, used to maintain privacy and anonymity.\n",
        "\n",
        "**-Screen Name:** Another code or identifier for the Twitter screen name of the user who posted the tweet, also used for privacy reasons.\n",
        "\n",
        "With this dataset, our objective is to develop a machine learning classification model that can analyze the tweet's content and accurately predict its sentiment as either positive, negative, or neutral. By leveraging the provided features, such as the tweet's text content, location, and timestamp, we aim to gain valuable insights into the overall sentiments expressed by Twitter users regarding COVID-19. This model could be valuable for understanding public sentiments during the pandemic and monitoring the emotional response to various events and situations related to COVID-19 on social media."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import Libraries\n",
        "import numpy as np                                  # Import NumPy for numerical operations\n",
        "import pandas as pd                                 # Import Pandas for data manipulation\n",
        "import matplotlib.pyplot as plt                     # Import Matplotlib for data visualization\n",
        "import seaborn as sns                               # Import Seaborn for enhanced data visualization\n",
        "import string                                      # Import string module for handling string operations\n",
        "from wordcloud import WordCloud                     # Import WordCloud for generating word clouds\n",
        "import nltk                                       # Import NLTK (Natural Language Toolkit) for NLP tasks\n",
        "nltk.download('all', quiet=True)                  # Downloading all NLTK data packages (quiet=True to suppress download messages)\n",
        "from PIL import Image                            # Import the Python Imaging Library (PIL) module for image processing\n",
        "\n",
        "# Model libraries for classification tasks\n",
        "from sklearn.metrics import *                   # Import various metrics for model evaluation\n",
        "from sklearn.model_selection import train_test_split  # Import train_test_split for splitting data into training and testing sets\n",
        "from sklearn.linear_model import LogisticRegression  # Import LogisticRegression for logistic regression modeling\n",
        "from sklearn.tree import DecisionTreeClassifier     # Import DecisionTreeClassifier for decision tree modeling\n",
        "from xgboost import XGBClassifier                 # Import XGBClassifier for XGBoost (extreme gradient boosting) modeling\n",
        "from sklearn.model_selection import GridSearchCV    # Import GridSearchCV for hyperparameter tuning using cross-validation\n",
        "from sklearn.svm import SVC                       # Import SVC for support vector machine modeling\n",
        "from sklearn.neighbors import KNeighborsClassifier # Import KNeighborsClassifier for k-nearest neighbors modeling\n",
        "\n",
        "# Metrics and evaluation libraries\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # Import confusion_matrix and classification_report for model evaluation\n",
        "from sklearn.model_selection import cross_val_score                    # Import cross_val_score for cross-validation of models\n",
        "\n",
        "# Suppressing warnings\n",
        "import warnings                                     # Import warnings module to suppress warning messages\n",
        "warnings.filterwarnings('ignore')                    # Ignore any warning messages generated during the code execution"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rakshanda\n",
        "tweet_df = pd.read_csv('/content/Coronavirus Tweets.csv',encoding='latin-1')"
      ],
      "metadata": {
        "id": "FrMqRMPeZvfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "tweet_df =pd.read_csv('/content/drive/MyDrive/Coronavirus Tweets.csv' ,encoding='latin-1')\n"
      ],
      "metadata": {
        "id": "WPT7hNxlIAmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First\n",
        "# Display the first few rows of the DataFrame with a cool color gradient\n",
        "# This code uses the 'head()' method to display the first few rows of the DataFrame.\n",
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df.tail()"
      ],
      "metadata": {
        "id": "L-rGfYIYbjrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "# Get the shape of the DataFrame\n",
        "tweet_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the column names of the DataFrame\n",
        "tweet_df.columns"
      ],
      "metadata": {
        "id": "L3WLfJQMb0c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# Dataset Info\n",
        "# Display concise summary of the DataFrame\n",
        "tweet_df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "# Dataset Duplicate Value Count\n",
        "# Check for duplicate rows in the DataFrame\n",
        "num_duplicates = tweet_df.duplicated().sum()\n",
        "num_duplicates"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of duplicate rows in the DataFrame\n",
        "num_duplicates = len(tweet_df) - len(tweet_df.drop_duplicates())\n",
        "\n",
        "# Create a bar plot to showcase the count of duplicate and unique rows\n",
        "# - Set the figure size to 8x6 inches\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Use seaborn's barplot function to create the bar plot\n",
        "# - The x-axis will display two categories: \"Duplicate Rows\" and \"Unique Rows\"\n",
        "# - The y-axis will show the corresponding counts of duplicate and unique rows\n",
        "sns.barplot(x=[\"Duplicate Rows\", \"Unique Rows\"], y=[num_duplicates, len(tweet_df) - num_duplicates])\n",
        "\n",
        "# Set labels for the x-axis and y-axis\n",
        "plt.xlabel(\"Row Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# Set the title of the bar plot\n",
        "plt.title(\"Count of Duplicate and Unique Rows\")\n",
        "\n",
        "# Display the bar plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ohk4ilcZcExb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used in the analysis is free from duplicate values. All the rows in the dataset are unique, and there are no instances where multiple rows have identical data across all columns. As a result, the dataset provides distinct and non-repetitive information, ensuring that each observation contributes uniquely to the analysis without any duplications."
      ],
      "metadata": {
        "id": "c1XPzP9pcMS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values_count = tweet_df.isna().sum()\n",
        "missing_values_count\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Create a bar plot to showcase the count of missing values in each column\n",
        "plt.figure(figsize=(12, 6))  # Set the size of the figure for better visualization\n",
        "\n",
        "# Use the seaborn 'barplot()' function to create the bar plot\n",
        "# x=missing_values_count.index: Set the x-axis labels using column names (index of the 'missing_values_count' Series).\n",
        "# y=missing_values_count.values: Set the y-axis values using the count of missing values for each column.\n",
        "sns.barplot(x=missing_values_count.index, y=missing_values_count.values)\n",
        "\n",
        "plt.xlabel(\"Columns\")  # Set the x-axis label to \"Columns\" for column names\n",
        "plt.ylabel(\"Missing Values Count\")  # Set the y-axis label to \"Missing Values Count\" for count values\n",
        "plt.title(\"Count of Missing Values in Each Column\")  # Add a title to the plot\n",
        "plt.xticks(rotation=90)  # Rotate the x-axis labels by 90 degrees for better readability when there are many columns\n",
        "plt.show()  # Display the bar plot"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset, there are six columns: 'Location', 'UserName', 'ScreenName', 'TweetAt', 'OriginalTweet', and 'Sentiment'. Among these columns, the 'Location' column contains duplicated values, with a total of 8590 duplicate entries. Identifying and handling these duplicate values will be important for our classification machine learning project to ensure the data is clean and accurate before proceeding with the analysis."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "# Get the column names of the DataFrame\n",
        "columns = tweet_df.columns\n",
        "columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# Generate descriptive statistics for the DataFrame, including all data types (include='all')\n",
        "description = tweet_df.describe(include='all')\n",
        "description"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset under consideration contains information about tweets related to the Coronavirus pandemic. It includes valuable attributes extracted from the tweets, which can aid in understanding public sentiments, opinions, and geographical distribution during the pandemic. The dataset consists of several columns, each holding specific information about the tweets and the users who posted them. The columns include UserName, ScreenName, Location, TweetAt, OriginalTweet, and Sentiment. Let's explore each of these columns to gain insights into the data and better comprehend the characteristics of the tweets in this dataset:\n",
        "\n",
        "UserName: This column contains the username of the person who authored the tweet on Twitter.\n",
        "\n",
        "ScreenName: This column contains the screen name or handle of the Twitter user who posted the tweet.\n",
        "\n",
        "Location: This column represents the location information provided by the Twitter user in their profile. It may include details about their city, state, country, or any other geographic location they have specified.\n",
        "\n",
        "TweetAt: This column stores the date and time when the tweet was posted on Twitter.\n",
        "\n",
        "OriginalTweet: This column contains the actual content of the tweet that was posted by the user.\n",
        "\n",
        "Sentiment: This column contains the sentiment label assigned to the tweet based on the results of a sentiment analysis algorithm. The sentiment label can indicate whether the tweet's content is positive, negative, extremely positive, extremely negative, or neutral, depending on the sentiment analysis results.\n",
        "\n",
        "The dataset's attributes offer a comprehensive view of the tweets, allowing us to explore public sentiments, identify tweet patterns, and potentially build a classification model to categorize tweets based on their sentiment."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Display unique values in each column of the DataFrame\n",
        "\n",
        "unique_values = tweet_df.apply(lambda col: col.unique())\n",
        "print(unique_values)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique sentiment labels present in the 'Sentiment' column and print them\n",
        "unique_sentiments = tweet_df['Sentiment'].unique()\n",
        "\n",
        "# Display the unique sentiment labels in the 'Sentiment' column\n",
        "print(\"Unique Sentiment Labels:\")\n",
        "print(unique_sentiments)"
      ],
      "metadata": {
        "id": "SM8a_QtBc5xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique locations present in the 'Location' column and print them\n",
        "unique_locations = tweet_df['Location'].unique()\n",
        "\n",
        "# Display the unique locations in the 'Location' column\n",
        "print(\"Unique Locations:\")\n",
        "print(unique_locations)\n"
      ],
      "metadata": {
        "id": "CwejSyFcc8sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are focusing on classifying tweets based on the text in the 'OriginalTweet' column, we do not need to remove null values from the 'Location' column. Retaining the null values in the 'Location' column ensures that we don't lose any potentially valuable information that could be useful for our classification model. Removing null values from 'Location' might lead to a loss of geographic information and could potentially impact the effectiveness of our model in understanding the relationship between tweet content and geographical location. Therefore, we choose to retain the null values in the 'Location' column to preserve all available data for our classification task."
      ],
      "metadata": {
        "id": "Yp3ENrFvdNMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the dataset's structure and contents is crucial for conducting a thorough analysis. To achieve this, several data preparation steps are taken, including:\n",
        "\n",
        "**Identifying Column Names:** The first step involves identifying the names of each column present in the dataset.\n",
        "\n",
        "**Describing Columns:** Each column is then described with pertinent details such as data type, possible value ranges, and other relevant information.\n",
        "\n",
        "**Defining Variables:** In this stage, the variables in the dataset are characterized, including their types (numerical, categorical, etc.) and their respective roles or functions.\n",
        "\n",
        "**Verifying Unique Values:** The dataset is checked for unique values within each column to identify any potential duplicates.\n",
        "\n",
        "**Managing Null Values:** It is noted that there is no need to remove or delete the empty values (null values), as they are retained in the analysis.\n",
        "\n",
        "By conducting these data preparation processes, we gain a comprehensive understanding of the dataset's attributes, enabling us to proceed with the analysis and build effective models for classification or any other desired tasks."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Univarient Analysis**"
      ],
      "metadata": {
        "id": "BHSievQjddQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 **Find peak tweet activity dates to reveal trends.(Bar Plot)**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Calculate tweet counts for each date\n",
        "tweet_counts = tweet_df['TweetAt'].value_counts().reset_index().rename(columns={'index': 'Tweet_Date', 'TweetAt': 'Count'})\n",
        "\n",
        "# Sort the tweet counts in descending order to find the top 20 dates\n",
        "top_tweet_dates = tweet_counts.sort_values(by='Count', ascending=False).head(20)\n",
        "\n",
        "# Create a figure and set its size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create a bar plot to visualize the top 20 tweet dates\n",
        "# x-axis will represent the tweet count (Count), y-axis will represent the date (Tweet_Date)\n",
        "# data=top_tweet_dates specifies the data to be used for the plot\n",
        "# palette='viridis' sets the color scheme for the bars\n",
        "sns.barplot(x='Count', y='Tweet_Date', data=top_tweet_dates, palette='viridis')\n",
        "\n",
        "# Set the label for the x-axis\n",
        "plt.xlabel('Number of Tweets')\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel('Date')\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title('Top 20 Most-Tweeted-About Dates of the Year')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We analyzed the tweet data to determine the most-tweeted-about dates of the year. By calculating tweet counts for each date and sorting them in descending order, we identified the top 20 dates with the highest tweet activity. The results were visualized using a bar plot, presenting the tweet counts on the x-axis and the corresponding dates on the y-axis. The bar plot's ranking order allowed us to easily recognize the most significant dates with the highest tweet volumes. With a limited number of categories, the bar plot provided a clear and concise representation of the data. The use of the 'viridis' color palette enhanced the visualization's aesthetics and readability, enabling us to gain insights into peak activity periods and potential trends related to the tweets."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our dataset, we have analyzed the tweets to identify the specific day that witnessed the highest tweet activity. By calculating the total number of tweets posted on each date and then sorting them in descending order, we were able to pinpoint the day that received the most tweets. This analysis provides valuable insights into the peak engagement periods, allowing us to understand when users were most active in posting tweets related to the topic under consideration. Identifying the day with the highest tweet volume can be significant for understanding public sentiments, trending topics, and potential events that might have triggered increased social media activity. It enables us to focus on specific dates for further investigation, facilitating deeper exploration into the reasons behind the heightened tweet activity and its implications."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from identifying peak tweet activity can be leveraged to improve social media strategies, enhance customer engagement, and capitalize on opportunities for positive growth. At the same time, it can help businesses address potential negative issues and manage their brand reputation effectively. The significance of the impact depends on how businesses utilize and respond to the insights within their broader marketing and operational strategies."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 **- Sentiment-wise tweet count(Bar Plot)**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, we are dealing with a five-class classification problem for sentiment analysis, where the sentiments can be extremely positive, positive, neutral, negative, or extremely negative. However, to simplify the problem and create a more manageable three-class classification task, we will group the extremely positive and extremely negative tweets together into a single class. This means that the new three-class classification problem will consist of positive, neutral, and negative sentiments, effectively merging the extreme categories into their respective positive and negative classes."
      ],
      "metadata": {
        "id": "Dz99d6EHeWFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Replacing these values 'Extremely Negative' : 'Negative', 'Extremely Positive' : 'Positive'\n",
        "tweet_df['Sentiment'] = tweet_df['Sentiment'].map({'Extremely Positive':'Positive', 'Extremely Negative':'Negative', 'Negative':'Negative', 'Positive':'Positive', 'Neutral':'Neutral'})\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of tweets for each sentiment type and store it in a new DataFrame\n",
        "sentiment_counts = tweet_df['Sentiment'].value_counts().reset_index().rename(columns={'index': 'Sentiment Types', 'Sentiment': 'Counts'})\n",
        "\n",
        "# Define custom colors for each sentiment type\n",
        "colors = ['#77DD77', '#FF6961', '#AEC6CF']  # Green, Red, Blue\n",
        "\n",
        "# Plot the sentiment breakup using a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a bar chart with sentiment types on the x-axis and their corresponding counts on the y-axis\n",
        "# color=colors sets the custom colors for each sentiment type\n",
        "plt.bar(sentiment_counts['Sentiment Types'], sentiment_counts['Counts'], color=colors)\n",
        "\n",
        "# Set the label for the x-axis\n",
        "plt.xlabel('Sentiment Types', fontsize=12)\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel('Counts', fontsize=12)\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title('Sentiment Breakup of Tweets', fontsize=16)\n",
        "\n",
        "# Adjust the appearance of x-axis labels: fontsize=11, rotation=90 (to make them vertical), ha='right' (align them to the right)\n",
        "plt.xticks(fontsize=11, rotation=90, ha='right')\n",
        "\n",
        "# Adjust the appearance of y-axis labels: fontsize=11\n",
        "plt.yticks(fontsize=11)\n",
        "\n",
        "# Adjust the layout to avoid cropping labels or titles\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cgzN6zYjejdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific bar chart was chosen due to its suitability for representing categorical data, making it ideal for displaying sentiment categories like \"Positive,\" \"Negative,\" and \"Neutral.\" The chart allows for easy comparison of sentiment quantities, enabling quick identification of the most prevalent sentiment type. With clear labeling of axes, the bar chart ensures that the audience can easily understand the data. Moreover, its readability and space efficiency make it an effective choice for displaying multiple categories. By presenting the counts of each sentiment type side by side, the bar chart provides valuable insights into the sentiment distribution in the dataset, contributing to a better understanding of the data."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph depicts the sentiment trend in tweets regarding a particular subject, offering insights into the public's overall viewpoint. Positive sentiments reflect favorable opinions, while negative sentiments indicate unfavorable ones. The graph helped analyze the public's sentiment towards the subject, revealing that the majority of tweets expressed positive feelings, while the least number of tweets had a neutral attitude."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights from the sentiment analysis can indeed have a positive business impact. Knowing that the majority of tweets express positive feelings towards the subject can be beneficial for businesses in various ways:\n",
        "\n",
        "**Brand Perception:** Positive sentiments indicate a favorable perception of the brand or subject. This can enhance the brand's reputation and credibility among the target audience, leading to increased customer trust and loyalty.\n",
        "\n",
        "**Customer Satisfaction:** Positive opinions may indicate that customers are satisfied with the products or services related to the subject. Understanding this can help businesses identify what aspects are resonating well with customers and further improve their offerings.\n",
        "\n",
        "**Marketing and Communication:** Knowing the positive sentiments allows businesses to focus on highlighting the aspects that customers appreciate the most in their marketing and communication strategies. This targeted approach can lead to more effective campaigns.\n",
        "\n",
        "Regarding insights that may lead to negative growth, it is possible that negative sentiments might arise for various reasons:\n",
        "\n",
        "**Customer Complaints:** Negative sentiments may be a result of customer dissatisfaction or complaints. Identifying these negative opinions can help businesses address issues promptly and improve customer experience.\n",
        "\n",
        "**Product or Service Issues:** Negative sentiments might indicate potential problems or flaws in products or services related to the subject. Addressing these issues can prevent further negative feedback and improve overall product quality.\n",
        "\n",
        "**Competitive Analysis:** Analyzing negative sentiments can also provide insights into areas where competitors might be performing better. This can help businesses identify areas of improvement and stay competitive in the market.\n",
        "\n",
        "It's essential to carefully analyze and address negative sentiments to avoid any adverse impact on the business. By understanding and acting upon both positive and negative insights, businesses can make informed decisions and work towards achieving a positive business impact."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - **Compute sentiment class percentages precisely.(Pie Chart)**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Count the number of tweets for each sentiment type\n",
        "sentiment_count = tweet_df['Sentiment'].value_counts().to_list()\n",
        "\n",
        "# Define the labels for sentiment categories\n",
        "labels = ['Positive', 'Negative', 'Neutral']\n",
        "\n",
        "# Define custom colors for each sentiment category\n",
        "colors = ['#66c2a5', '#fc8d62', '#8da0cb']  # Bluish green, Orange, Light blue\n",
        "\n",
        "# Set figure size and font size\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.rcParams['font.size'] = 14\n",
        "\n",
        "# Explode the \"Neutral\" slice to highlight it\n",
        "explode = [0.04, 0.04, 0.1]\n",
        "\n",
        "# Plot the pie chart with shadow effect and percentage labels inside the pie slices\n",
        "wedges, texts, autotexts = plt.pie(x=sentiment_count, explode=explode, labels=labels, autopct=\"%.2f%%\", colors=colors, startangle=140, wedgeprops={'edgecolor': 'black'}, shadow=True, textprops={'fontsize': 12})\n",
        "\n",
        "# Add a title and legend with better positioning\n",
        "plt.title(\"Proportion Of Sentiments\", fontsize=20)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "\n",
        "# Equal aspect ratio ensures the pie chart is drawn as a circle.\n",
        "plt.axis('equal')\n",
        "\n",
        "# Make the percentage labels inside the pie chart bold\n",
        "for autotext in autotexts:\n",
        "    autotext.set_weight('bold')\n",
        "\n",
        "# Add a description text below the pie chart\n",
        "description = \"Sentiment distribution of tweets\\nPositive, Negative, and Neutral\"\n",
        "plt.text(0.5, -0.15, description, fontsize=12, color='black', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart was selected as the visualization for the sentiment distribution of tweets due to its ability to compare parts to the whole. It clearly represents the proportion of positive, negative, and neutral sentiments in the dataset through labeled slices, making it easy to grasp the sentiment breakdown. The visual appeal of the pie chart aids in intuitively understanding the sentiment distribution. With only three sentiment categories, the pie chart is simple, concise, and visually engaging, allowing for easy interpretation of the data. Moreover, by using the \"explode\" parameter, we can highlight the \"Neutral\" sentiment, drawing attention to its presence or significance within the dataset. Overall, the pie chart effectively communicates the sentiment trends in a visually appealing and informative manner."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After merging Extremely Positive and Extremely Negative sentiments into Positive and Negative sentiments, respectively, the Pie-Plot still shows a significantly higher number of \"Positive\" sentiments. This indicates a prevalent positive sentiment towards the subject in the majority of tweets. Further analysis can uncover insights into the factors driving this positivity and provide valuable information for businesses to understand public perception. The Pie-Plot offers a concise and visual representation of the sentiment distribution, making it easier to interpret the overall sentiment trends in the dataset."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the insights gained from sentiment analysis can guide businesses in making informed decisions, improving their products/services, and tailoring their strategies to capitalize on positive sentiment trends. However, businesses should also be vigilant about addressing any negative insights to ensure sustained growth and customer satisfaction."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - **Scatter Geo Plot and Bar Plot - Identifying the geographical locations from where the tweets were sent.**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# importing libraraies\n",
        "import plotly.express as px\n",
        "\n",
        "# Filter the data to exclude tweets with unknown location and count total tweets from each location\n",
        "location_counts = tweet_df[tweet_df['Location'] != 'Unknown']['Location'].value_counts()\n",
        "\n",
        "# Create a DataFrame to store location and tweet counts\n",
        "location_df = location_counts.reset_index()\n",
        "location_df.columns = ['Location', 'Total Tweets']\n",
        "\n",
        "# Plot the globe graph with a different color scale\n",
        "fig = px.scatter_geo(location_df, locations='Location', locationmode='country names', size='Total Tweets',\n",
        "                     hover_name='Location', projection='natural earth', title='Total Tweets by Location',\n",
        "                     color='Total Tweets', color_continuous_scale='Reds')\n",
        "# Display the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import plotly.express as px\n",
        "\n",
        "# Filter the data to exclude tweets with unknown location and count total tweets from each location\n",
        "location_counts = tweet_df[tweet_df['Location'] != 'Unknown']['Location'].value_counts()\n",
        "\n",
        "# Create a DataFrame to store location and tweet counts\n",
        "location_df = location_counts.reset_index()\n",
        "location_df.columns = ['Location', 'Total Tweets']\n",
        "\n",
        "# Sort the DataFrame in descending order of tweet counts for better visualization\n",
        "location_df = location_df.sort_values(by='Total Tweets', ascending=False)\n",
        "\n",
        "# Select only the top 10 cities with the highest tweet counts\n",
        "top_20_cities = location_df.head(20)\n",
        "\n",
        "# Create a bar graph using Plotly with multiple colors\n",
        "fig = px.bar(top_20_cities, x='Location', y='Total Tweets', color='Total Tweets',\n",
        "             color_continuous_scale='Reds', title='Top 20 Cities by Total Tweets')\n",
        "\n",
        "# Customize the appearance of the graph\n",
        "fig.update_layout(xaxis_title='Location',\n",
        "                  yaxis_title='Total Tweets',\n",
        "                  xaxis_tickangle=-45,  # Rotate x-axis labels for better visibility\n",
        "                  showlegend=False     # Hide legend since there's only one bar\n",
        "                  )\n",
        "# Display the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Fc54TQcAgH97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualizations consist of two charts. The Scatter Geo Chart uses scatter_geo to display tweet counts across various geographic locations, with the size and color of points indicating activity levels. The 'Reds' color scale signifies higher tweet counts with darker shades. This chart provides a clear geographic representation, offering insights into the distribution of tweets across different regions.\n",
        "\n",
        "The second visualization is a Bar Chart, created using bar, showcasing the top 10 cities with the highest tweet counts. The bars are color-coded based on tweet counts using the 'Reds' color scale. This bar chart enables easy comparison of tweet counts among cities, emphasizing the top-performing locations. The x-axis denotes cities, while the y-axis represents the total tweet counts.\n",
        "\n",
        "Both visualizations provide valuable insights into Twitter activity, combining geographic spread and top-performing cities to offer a comprehensive understanding of the data's distribution and engagement patterns."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that London, United States, and London, England have the highest tweet counts, indicating significant Twitter activity from these locations. US cities New York, NY, and Washington, DC, also show notable tweet engagement. The insights highlight a geographically diverse audience discussing the subject on Twitter, presenting opportunities for targeted content and campaigns to maximize engagement. Businesses can use this data to understand where the subject gains traction and focus efforts accordingly. The high tweet counts suggest active interest and engagement in the subject among Twitter users from these regions."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can positively impact businesses by enabling targeted marketing, enhanced customer engagement, and an understanding of the diverse audience. However, to ensure positive growth, businesses should carefully consider sentiment analysis, content relevance, and inclusiveness in their strategies."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 -  **Scatter Geo Plot and Bar Plot - Determining the location with the most favourable thoughts Tweets were sent out.**\n"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# importing libraraies\n",
        "import plotly.express as px\n",
        "\n",
        "# Filter the data to exclude tweets with unknown location and count total tweets from each location\n",
        "positive_tweets = tweet_df[(tweet_df[\"Sentiment\"] == \"Positive\") & (tweet_df['Location'] != 'Unknown')]\n",
        "location_counts = positive_tweets['Location'].value_counts()\n",
        "\n",
        "# Create a DataFrame to store location and tweet counts\n",
        "location_df = location_counts.reset_index()\n",
        "location_df.columns = ['Location', 'positive Tweets']\n",
        "\n",
        "# Plot the globe graph with a different color scale\n",
        "fig = px.scatter_geo(location_df, locations='Location', locationmode='country names', size='positive Tweets',\n",
        "                     hover_name='Location', projection='natural earth', title='positive Tweets by Location',\n",
        "                     color='positive Tweets', color_continuous_scale='Reds')\n",
        "# Display the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import plotly.express as px\n",
        "\n",
        "# Filter the data to exclude tweets with unknown location and count positive tweets from each location\n",
        "positive_tweets = tweet_df[(tweet_df[\"Sentiment\"] == \"Positive\") & (tweet_df['Location'] != 'Unknown')]\n",
        "location_counts = positive_tweets['Location'].value_counts()\n",
        "\n",
        "\n",
        "# Create a DataFrame to store location and tweet counts\n",
        "location_df = location_counts.reset_index()\n",
        "location_df.columns = ['Location', 'positive Tweets']\n",
        "\n",
        "# Sort the DataFrame in descending order of tweet counts for better visualization\n",
        "location_df = location_df.sort_values(by='positive Tweets', ascending=False)\n",
        "\n",
        "# Select only the top 10 cities with the highest tweet counts\n",
        "top_20_cities = location_df.head(20)\n",
        "\n",
        "# Create a bar graph using Plotly with multiple colors\n",
        "fig = px.bar(top_20_cities, x='Location', y='positive Tweets', color='positive Tweets',\n",
        "             color_continuous_scale='Reds', title='Top 20 Cities by positive Tweets')\n",
        "\n",
        "# Customize the appearance of the graph\n",
        "fig.update_layout(xaxis_title='Location',\n",
        "                  yaxis_title='positive Tweets',\n",
        "                  xaxis_tickangle=-45,  # Rotate x-axis labels for better visibility\n",
        "                  showlegend=False     # Hide legend since there's only one bar\n",
        "                  )\n",
        "\n",
        "# Display the figure\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "gVvB4Z04guPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen horizontal bar chart effectively compares positive tweet counts from different locations (excluding 'Unknown'). It displays the top 20 locations with the highest tweet counts, using the 'viridis' color palette for visual appeal. The Seaborn style 'whitegrid' enhances the plot's readability. The chart's layout efficiently uses space and accommodates location labels well. Its descending order helps highlight the most significant locations. The chart provides quick insights into locations with the highest positive tweet activity."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data analysis indicates that the United States has the most positive tweets among all the locations, suggesting a prevailing positive mood on Twitter from that region. London and England closely follow the United States, showing a substantial number of positive tweets as well. In contrast, Global and Boston, Massachusetts have the fewest favorable tweets, implying a comparatively lower overall positive sentiment on the platform from these areas. The findings highlight regional variations in Twitter users' mood and sentiment, with some locations exhibiting higher levels of positivity while others have a lesser positive engagement. Further exploration of the reasons behind these variations could provide valuable insights into the social and cultural aspects influencing online sentiment in different geographical regions."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights from the data analysis can potentially help create a positive business impact in several ways:\n",
        "\n",
        "**Targeted Marketing:** Knowing the regions with the highest number of positive tweets (e.g., United States, London, England) can aid businesses in targeting their marketing efforts towards these locations. They can tailor their campaigns to resonate with the positive sentiment of users in these regions, potentially leading to increased engagement and brand loyalty.\n",
        "\n",
        "**Reputation Management:** Understanding the regions with the lowest favorable tweets (e.g., Global and Boston, Massachusetts) can prompt businesses to focus on reputation management strategies in these areas. By addressing any negative sentiments or concerns specific to these locations, they can work towards improving their brand perception and customer satisfaction.\n",
        "\n",
        "**Product Launches:** Businesses can leverage the insights to strategically launch new products or services in regions with high positive engagement. Positive sentiment can generate more buzz and enthusiasm for new offerings, leading to a potentially successful product launch.\n",
        "\n",
        "**Customer Insights:** Analyzing the regional variations in sentiment can provide valuable customer insights. Understanding what drives positive sentiment in different locations can help businesses identify customer preferences and tailor their offerings accordingly.\n",
        "\n",
        "As for insights that may lead to negative growth, one potential scenario could be if a particular region consistently shows a negative sentiment towards a specific product, service, or brand. This negative sentiment could indicate issues with the business's offerings, customer service, or reputation in that region. If not addressed promptly and effectively, it could lead to decreased customer loyalty, decreased sales, and negative business impact in that area.\n",
        "\n",
        "It is essential for businesses to pay attention to negative sentiment and take proactive steps to address any underlying issues. Responding to customer concerns, improving product quality, and engaging with dissatisfied customers can help mitigate negative growth and turn the situation around. However, if these negative sentiments persist without appropriate actions, it could lead to a decline in business performance in that particular region."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - **Scatter Geo Plot and Bar - Plot Determining the location with the neutral thoughts Tweets were sent out.**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# importing libraraies\n",
        "import plotly.express as px\n",
        "\n",
        "# Filter the data to exclude tweets with unknown location and count total tweets from each location\n",
        "Neutral_tweets = tweet_df[(tweet_df[\"Sentiment\"] == \"Neutral\") & (tweet_df['Location'] != 'Unknown')]\n",
        "location_counts = Neutral_tweets['Location'].value_counts()\n",
        "\n",
        "# Create a DataFrame to store location and tweet counts\n",
        "location_df = location_counts.reset_index()\n",
        "location_df.columns = ['Location', 'Neutral Tweets']\n",
        "\n",
        "# Plot the globe graph with a different color scale\n",
        "fig = px.scatter_geo(location_df, locations='Location', locationmode='country names', size='Neutral Tweets',\n",
        "                     hover_name='Location', projection='natural earth', title='Neutral Tweets by Location',\n",
        "                     color='Neutral Tweets', color_continuous_scale='Reds')\n",
        "# Display the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import plotly.express as px\n",
        "\n",
        "# Filter the data to exclude tweets with unknown location and count positive tweets from each location\n",
        "Neutral_tweets = tweet_df[(tweet_df[\"Sentiment\"] == \"Neutral\") & (tweet_df['Location'] != 'Unknown')]\n",
        "location_counts = Neutral_tweets['Location'].value_counts()\n",
        "\n",
        "# Create a DataFrame to store location and tweet counts\n",
        "location_df = location_counts.reset_index()\n",
        "location_df.columns = ['Location', 'Neutral Tweets']\n",
        "\n",
        "# Sort the DataFrame in descending order of tweet counts for better visualization\n",
        "location_df = location_df.sort_values(by='Neutral Tweets', ascending=False)\n",
        "\n",
        "# Select only the top 10 cities with the highest tweet counts\n",
        "top_20_cities = location_df.head(20)\n",
        "\n",
        "# Create a bar graph using Plotly with multiple colors\n",
        "fig = px.bar(top_20_cities, x='Location', y='Neutral Tweets', color='Neutral Tweets',\n",
        "             color_continuous_scale='Reds', title='Top 20 Cities by Neutral Tweets')\n",
        "\n",
        "# Customize the appearance of the graph\n",
        "fig.update_layout(xaxis_title='Location',\n",
        "                  yaxis_title='Neutral Tweets',\n",
        "                  xaxis_tickangle=-45,  # Rotate x-axis labels for better visibility\n",
        "                  showlegend=False     # Hide legend since there's only one bar\n",
        "                  )\n",
        "# Display the figure\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "O9Wy8NdUhcAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific horizontal bar chart was chosen for its ability to efficiently compare neutral tweet counts from various locations. It displays the top 20 locations with the highest neutral tweet counts in descending order, aiding in clear identification of significant locations. The horizontal layout optimizes space and ensures legible location labels. The 'viridis' color palette enhances visual appeal and distinguishes between bars. The Seaborn style 'whitegrid' adds a clean and professional look. Overall, the chart effectively presents insights into the distribution of neutral sentiments across different regions."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals geographical diversity with top locations spanning various countries and cities, including the United States, England, India, Canada, and Australia. Major urban centers like London, New York, Los Angeles, and San Francisco show high neutral tweet engagement. The United States and London, England, stand out with the highest neutral tweet counts. Some locations have specific mentions in both country and city formats, leading to multiple entries. \"Worldwide\" appears in the list, indicating global neutral discussions. Businesses can potentially target regions with high neutral tweet counts for relevant campaigns. The chart provides valuable insights into user behavior and sentiment patterns in different regions on Twitter."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the chart can be valuable in formulating effective marketing strategies, understanding user behavior, and making data-driven decisions to foster a positive business impact. However, addressing negative sentiment in regions with consistent negative engagement is essential to avoid any potential negative consequences on business growth and performance."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 **Scatter Geo Plot and Bar - Determining the location with the negative thoughts Tweets were sent out**\n"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# importing libraraies\n",
        "import plotly.express as px\n",
        "\n",
        "# Filter the data to exclude tweets with unknown location and count Negative tweets from each location\n",
        "Negative_tweets = tweet_df[(tweet_df[\"Sentiment\"] == \"Negative\") & (tweet_df['Location'] != 'Unknown')]\n",
        "location_counts = Neutral_tweets['Location'].value_counts()\n",
        "\n",
        "# Create a DataFrame to store location and tweet counts\n",
        "location_df = location_counts.reset_index()\n",
        "location_df.columns = ['Location', 'Negative Tweets']\n",
        "\n",
        "# Plot the globe graph with a different color scale\n",
        "fig = px.scatter_geo(location_df, locations='Location', locationmode='country names', size='Negative Tweets',\n",
        "                     hover_name='Location', projection='natural earth', title='Negative Tweets by Location',\n",
        "                     color='Negative Tweets', color_continuous_scale='Reds')\n",
        "# Display the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import plotly.express as px\n",
        "\n",
        "# Filter the data to exclude tweets with unknown location and count Negative tweets from each location\n",
        "Negative_tweets = tweet_df[(tweet_df[\"Sentiment\"] == \"Negative\") & (tweet_df['Location'] != 'Unknown')]\n",
        "location_counts = Negative_tweets['Location'].value_counts()\n",
        "\n",
        "# Create a DataFrame to store location and tweet counts\n",
        "location_df = location_counts.reset_index()\n",
        "location_df.columns = ['Location', 'Negative Tweets']\n",
        "\n",
        "# Sort the DataFrame in descending order of tweet counts for better visualization\n",
        "location_df = location_df.sort_values(by='Negative Tweets', ascending=False)\n",
        "\n",
        "# Select only the top 10 cities with the highest tweet counts\n",
        "top_20_cities = location_df.head(20)\n",
        "\n",
        "# Create a bar graph using Plotly with multiple colors\n",
        "fig = px.bar(top_20_cities, x='Location', y='Negative Tweets', color='Negative Tweets',\n",
        "             color_continuous_scale='Reds', title='Top 20 Cities by Negative Tweets')\n",
        "\n",
        "# Customize the appearance of the graph\n",
        "fig.update_layout(xaxis_title='Location',\n",
        "                  yaxis_title='Negative Tweets',\n",
        "                  xaxis_tickangle=-45,  # Rotate x-axis labels for better visibility\n",
        "                  showlegend=False     # Hide legend since there's only one bar\n",
        "                  )\n",
        "# Display the figure\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "gyZT_MhIiDby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific horizontal bar chart was chosen to efficiently compare negative tweet counts from different locations. It displays the top 20 locations with the highest negative tweet counts in descending order, making it easy to identify significant locations. The horizontal layout optimizes space and ensures clear location labels. The 'viridis' color palette enhances visual appeal and distinguishes between bars. The Seaborn style 'whitegrid' adds a clean and professional look. Overall, the chart provides valuable insights into the distribution of negative sentiments across various regions on Twitter."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows a diverse representation of negative sentiments on Twitter across various countries and cities. Major urban centers like London, Washington, New York, and Los Angeles are active in expressing negative tweets. Both the United Kingdom and the United States have significant negative tweet volumes. Some locations have multiple mentions with both country and city names, leading to separate entries. \"Global\" represents negative discussions without specific location ties. International engagement is evident with locations like India and Australia in the list. The chart highlights potential concerns for businesses operating in high negative tweet count areas. It provides valuable insights into user behavior and sentiment patterns in different regions on Twitter."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, while the insights gained from the chart can help businesses identify opportunities for targeted reputation management and customer engagement strategies, they also highlight potential challenges related to negative sentiment in specific regions. Proactive measures to address negative sentiments and improve brand perception can contribute to a positive business impact, while neglecting these concerns may lead to negative growth implications."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - **Bar Plot and Word Cloud - Top 50 Hashtags**"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Import necessary libraries\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define a function to count all the hashtags from the tweets\n",
        "def hashtag_count(tweets):\n",
        "    ''' This function counts all the hashtags from the tweets '''\n",
        "    hashtags_list = []\n",
        "\n",
        "    # Loop through each tweet\n",
        "    for tweet in tweets:\n",
        "        # Find all the hashtags in the tweet using regular expression\n",
        "        hashtags = re.findall(r'#(\\w+)', tweet)\n",
        "        # Append the hashtags to the list\n",
        "        hashtags_list.extend(hashtags)\n",
        "\n",
        "    return hashtags_list\n",
        "\n",
        "# Call the hashtag_count function with the 'OriginalTweet' column of the 'tweet_df' DataFrame\n",
        "# to get a list of all hashtags from all tweets\n",
        "total_hashtags = hashtag_count(tweet_df['OriginalTweet'])\n",
        "\n",
        "# Create a DataFrame to store all the hashtags\n",
        "total_hashtags_df = pd.DataFrame({\"Hashtags\": total_hashtags})\n",
        "\n",
        "# Calculate the total number of hashtags\n",
        "total_hashtags_count = len(total_hashtags_df)\n",
        "\n",
        "# Calculate the number of unique hashtags\n",
        "unique_hashtags_count = total_hashtags_df.nunique()\n",
        "\n",
        "# Calculate the top 50 hashtags with their counts\n",
        "top_hashtags = total_hashtags_df['Hashtags'].value_counts().reset_index().rename(columns={'index': 'Hashtag', 'Hashtags': 'Number_of_Hashtags'}).head(50)\n",
        "\n",
        "# Set a beautiful Seaborn style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot the bar chart with the 'viridis' color palette\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.barplot(x='Number_of_Hashtags', y='Hashtag', data=top_hashtags, palette='viridis')\n",
        "\n",
        "# Set the label for the x-axis\n",
        "plt.xlabel('Number of Hashtags', fontsize=16)\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel('Hashtag', fontsize=19)\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title('Top 50 Hashtags in Tweets', fontsize=20)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Join all words from the 'tweets' DataFrame into a single string\n",
        "all_words = ' '.join([str(text) for text in tweet_df['OriginalTweet']])\n",
        "\n",
        "# Create a WordCloud object with specified parameters\n",
        "wordcloud = WordCloud(\n",
        "    width=800,                    # Width of the word cloud image\n",
        "    height=500,                   # Height of the word cloud image\n",
        "    stopwords=set(STOPWORDS),     # Set of stopwords to be excluded from the word cloud\n",
        "    background_color=\"white\",     # Background color of the word cloud image\n",
        "    random_state=21,              # Random state for reproducibility\n",
        "    max_font_size=110             # Maximum font size for the most frequent word\n",
        ").generate(all_words)\n",
        "\n",
        "# Display the generated word cloud image\n",
        "plt.figure(figsize=(12, 8))       # Set the size of the plot\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")  # Show the word cloud with bilinear interpolation\n",
        "plt.axis('off')                   # Turn off the axis\n",
        "plt.show()                        # Display the word cloud plot\n"
      ],
      "metadata": {
        "id": "xhycKg1diuUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The selected horizontal bar chart efficiently compares the usage of hashtags in tweets. It displays the top 50 hashtags with the highest counts in descending order, enabling easy identification of the most popular hashtags. The horizontal layout optimizes space and ensures a clear representation of hashtag frequencies. The 'viridis' color palette enhances visual appeal and distinguishes between hashtags. The Seaborn style 'whitegrid' provides a clean and professional look to the chart. Overall, the chart offers valuable insights into the most frequently used hashtags in the dataset, allowing businesses and researchers to understand popular trends and topics on Twitter."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals insights about Twitter discussions during the COVID-19 pandemic. Hashtags related to COVID-19 dominate, including variations like \"coronavirus\" and \"COVID19.\" Supply and safety concerns are evident with hashtags like \"toiletpaper\" and \"panicbuying.\" Stay-at-home measures are reflected in hashtags like \"StayHomeSaveLives.\" Health and hygiene topics are present with hashtags like \"handsanitizer\" and \"sanitizer.\" Economic implications are indicated by hashtags like \"consumer\" and \"ecommerce.\" The chart also highlights discussions on the latest updates with hashtags like \"CoronaVirusUpdate\" and \"coronavirusuk.\" Overall, the chart provides valuable insights into public concerns, awareness, and social engagement during the pandemic."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the gained insights can positively impact businesses by enabling them to tailor their strategies, enhance customer engagement, and adapt to changing market demands. However, businesses should also be vigilant about potential challenges highlighted in the insights, such as economic implications and negative sentiments, to mitigate any adverse effects on their growth and performance during the COVID-19 pandemic. Proactive measures, responsive customer support, and thoughtful adaptations can help businesses navigate these challenges and foster a positive impact on their growth."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 -**Bar Plot - Top Months with the Highest Number of Tweets**"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "#Creating a column Day, Month & Week Day column\n",
        "tweet_df['day'] = pd.DatetimeIndex(tweet_df['TweetAt']).day\n",
        "tweet_df['month'] = pd.DatetimeIndex(tweet_df['TweetAt']).month\n",
        "tweet_df[\"week day\"] = pd.DatetimeIndex(tweet_df['TweetAt']).day_name()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of tweets in each month\n",
        "month_counts = tweet_df['month'].value_counts().reset_index().rename(columns={'index': 'month', 'month': 'counts'})\n",
        "\n",
        "# Get the top 2 months with the highest tweet counts\n",
        "top_months = month_counts.sort_values(by='counts', ascending=False)\n",
        "\n",
        "# Set the figure size for the plot\n",
        "plt.figure(figsize=(13, 9))\n",
        "\n",
        "# Create a bar plot using seaborn\n",
        "sns.barplot(data=top_months, x='month', y='counts', palette='viridis')\n",
        "\n",
        "# Set the title and labels for the plot\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.title('Top Months with the Highest Number of Tweets')\n",
        "\n",
        "# Customize the appearance of the plot\n",
        "sns.despine()  # Remove the top and right spines from the plot\n",
        "plt.xticks(rotation=0)  # Rotate the x-axis labels for better visibility\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "V7kuxMwzjZlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar plot is a simple yet effective way to visualize and compare tweet counts, identifying the top-performing months in tweet activity."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given data, the bar plot displays the tweet counts for two months: March (with 26,309 tweets) and April (with 2,479 tweets). The bar plot, using the 'viridis' color palette, effectively highlights the substantial difference in tweet activity between the top two months. This simple and visually clear representation allows for quick identification of the months with the highest tweet counts."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, the gained insights from the bar plot can lead to positive business impacts by capitalizing on high-performing months and optimizing resource allocation. However, it also highlights potential challenges in low-performing months, indicating the need for further analysis and strategic adjustments to avoid negative growth."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10  - **Bar Plot - Positive tweets per month**"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Convert the \"TweetAt\" column to a datetime object\n",
        "tweet_df['TweetAt'] = pd.to_datetime(tweet_df['TweetAt'])\n",
        "\n",
        "# Create a new column \"Month\" that contains the month of each tweet\n",
        "tweet_df['Month'] = tweet_df['TweetAt'].dt.month\n",
        "\n",
        "# Filter the tweets that have positive sentiment\n",
        "positive_tweet = tweet_df[tweet_df['Sentiment'] == 'Positive']\n",
        "\n",
        "# Count the number of positive tweets in each month\n",
        "positive_month_counts = positive_tweet['Month'].value_counts().reset_index().rename(columns={'index': 'Month', 'Month': 'Count'})\n",
        "\n",
        "# Sort the counts in descending order and select the top month\n",
        "top_month = positive_month_counts.sort_values(by='Count', ascending=False).iloc[0]\n",
        "\n",
        "# Set the width of the entire figure\n",
        "figure_width = 14  # Set the desired width in inches\n",
        "\n",
        "# Create a bar plot showing the number of positive tweets per month with a custom width\n",
        "# Set the size of the entire figure using figsize\n",
        "plt.figure(figsize=(figure_width, 6))\n",
        "\n",
        "# Create a bar plot with positive_month_counts DataFrame\n",
        "# x-axis represents the months, y-axis represents the count of positive tweets for each month\n",
        "# palette='viridis' sets the color scheme for the bars\n",
        "sns.barplot(data=positive_month_counts, x='Month', y='Count', palette='viridis')\n",
        "\n",
        "# Set the label for the x-axis\n",
        "plt.xlabel('Month')\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel('Number of positive tweets')\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title('Positive tweets per month')\n",
        "\n",
        "# Customize the outline by removing the top and right spines\n",
        "# This removes the right and top axes lines, making the plot cleaner\n",
        "sns.despine(top=True, right=True)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of chart should align with the data and the insights intended for the audience. For comparing quantities in categorical data, like positive tweets per month, bar plots are a practical and widely-used choice."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis of the chart shows that March had the highest number of positive tweets, followed by April, indicating a peak in positive sentiment during those months. Similarly, January had the highest number of positive tweets, followed by February, suggesting a similar pattern of positive sentiment during the earlier months. The insights suggest potential correlations with seasonal trends or events that influenced user sentiment on the platform during specific periods. Further analysis and contextual information could provide a deeper understanding of the reasons behind the variations in positive tweet counts across different months."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "The gained insights can potentially help create a positive business impact in the following ways:\n",
        "\n",
        "Targeted Marketing: Businesses can leverage the knowledge of peak positive sentiment months (March and April) and focus their marketing efforts during these periods. They can tailor their campaigns to capitalize on the increased positivity and engagement of users.\n",
        "\n",
        "Product Launches: Knowing the months with higher positive sentiment (January and February), businesses can strategically plan product launches or announcements during these periods to garner a more positive response from their audience.\n",
        "\n",
        "Customer Engagement: Understanding the seasonal trends and events that influence positive sentiment can guide businesses in creating relevant and engaging content that resonates with their target audience.\n",
        "\n",
        "Regarding negative growth insights, the chart alone does not directly indicate negative trends. It only shows the distribution of positive tweets over time. To identify potential negative growth insights, further analysis and additional data would be needed, such as:\n",
        "\n",
        "Negative Sentiment Analysis: Analyzing negative sentiment tweets over the same time frame to understand any patterns or correlations with specific months or events.\n",
        "\n",
        "Competitor Analysis: Comparing positive tweet trends with those of competitors to identify any possible negative growth for the business in comparison.\n",
        "\n",
        "Sentiment Shifts: Examining whether there are significant shifts in sentiment over time and investigating the causes behind them.\n",
        "\n",
        "Without such additional analysis, it is not possible to conclusively determine negative growth insights from the provided chart alone. It is essential to consider various factors and context to make accurate assessments of business impact and potential negative trends."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11-**Bar Plot - Negative tweets per month**"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Convert the \"TweetAt\" column to a datetime object\n",
        "tweet_df['TweetAt'] = pd.to_datetime(tweet_df['TweetAt'])\n",
        "\n",
        "# Create a new column \"Month\" that contains the month of each tweet\n",
        "tweet_df['Month'] = tweet_df['TweetAt'].dt.month\n",
        "\n",
        "# Filter the tweets that have Negative sentiment\n",
        "Negative_tweet = tweet_df[tweet_df['Sentiment'] == 'Negative']\n",
        "\n",
        "# Count the number of Negative tweets in each month\n",
        "Negative_month_counts = Negative_tweet['Month'].value_counts().reset_index().rename(columns={'index': 'Month', 'Month': 'Count'})\n",
        "\n",
        "# Sort the counts in descending order and select the top month\n",
        "top_month = Negative_month_counts.sort_values(by='Count', ascending=False).iloc[0]\n",
        "\n",
        "# Set the width of the entire figure\n",
        "figure_width = 14  # Set the desired width in inches\n",
        "\n",
        "# Create a bar plot showing the number of Negative tweets per month with a custom width\n",
        "# Set the size of the entire figure using figsize\n",
        "plt.figure(figsize=(figure_width, 6))\n",
        "\n",
        "# Create a bar plot with Negative_month_counts DataFrame\n",
        "# x-axis represents the months, y-axis represents the count of Negative tweets for each month\n",
        "# palette='viridis' sets the color scheme for the bars\n",
        "sns.barplot(data=Negative_month_counts, x='Month', y='Count', palette='viridis')\n",
        "\n",
        "# Set the label for the x-axis\n",
        "plt.xlabel('Month')\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel('Number of Negative tweets')\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title('Negative tweets per month')\n",
        "\n",
        "# Customize the outline by removing the top and right spines\n",
        "# This removes the right and top axes lines, making the plot cleaner\n",
        "sns.despine(top=True, right=True)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen chart, a Seaborn bar plot, effectively visualizes the number of negative tweets per month for the given dataset. Its suitability arises from:\n",
        "\n",
        "Effective representation of categorical data, with months on the x-axis and negative tweet counts on the y-axis.\n",
        "\n",
        "Facilitating comparison of tweet counts across different months through the height of bars.\n",
        "\n",
        "Readability and user-friendliness, making it accessible to both data analysts and general audiences.\n",
        "\n",
        "Flexibility for customization, enabling visual enhancements and label additions.\n",
        "\n",
        "Familiarity as a widely-used chart type, enhancing interpretability.\n",
        "\n",
        "By using the bar plot, the code concisely communicates insights on negative tweet trends over time, aiding potential analysis of seasonal patterns and events influencing negative sentiment."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis of the chart shows that March had the highest number of negative tweets, followed by April, indicating a peak in negative sentiment during those months. Similarly, January had the highest number of negative tweets, followed by February, suggesting a similar pattern of negative sentiment during the earlier months. The insights suggest potential correlations with seasonal trends or events that influenced user sentiment on the platform during specific periods. Further analysis and contextual information could provide a deeper understanding of the reasons behind the variations in negative tweet counts across different months."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights regarding the highest number of negative tweets in March and April, as well as January and February, can potentially have both positive and negative implications for a business, depending on how they are addressed:\n",
        "\n",
        "Positive Business Impact:\n",
        "\n",
        "Customer Feedback Improvement: Identifying the peak in negative sentiment months allows the business to focus on improving customer feedback and addressing potential issues during those periods. This can lead to better customer satisfaction and retention.\n",
        "\n",
        "Crisis Management: Recognizing patterns of negative sentiment can help businesses prepare for potential crises or negative events that may occur during specific months. Proactive crisis management can mitigate the impact on the brand's reputation.\n",
        "\n",
        "Negative Growth Insights:\n",
        "\n",
        "Product or Service Issues: Consistent negative sentiment during specific months may indicate recurring product or service issues. Failure to address these issues promptly could result in negative growth, impacting sales and customer loyalty.\n",
        "\n",
        "Seasonal Downturn: If negative sentiment is consistently higher during certain months, it may indicate a seasonal downturn in customer engagement or interest. Businesses need to strategize accordingly to counteract this trend and maintain growth.\n",
        "\n",
        "Overall, the insights gained from the analysis provide valuable information to guide business decisions and marketing strategies. By addressing and understanding the reasons behind negative sentiment during certain months, businesses can work to improve customer experience and potentially achieve positive business impact. However, failing to address recurring negative sentiment or not adapting to seasonal trends appropriately could lead to negative growth. Hence, a comprehensive approach to analyzing and acting upon the insights is crucial for maximizing positive impact and mitigating negative consequences."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bivarient Analysis**\n"
      ],
      "metadata": {
        "id": "90a591lFOo6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Bar Plot - Top 25 Locations with the Highest Tweet Counts by Sentiment"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Filter tweets with known locations\n",
        "loc1 = tweet_df[tweet_df['Location'] != 'Unknown']\n",
        "\n",
        "# Replace 'London, England' with 'London'\n",
        "loc1['Location'] = loc1['Location'].replace('London, England', 'London')\n",
        "\n",
        "# Group by Sentiment and Location to get tweet counts for each location\n",
        "location_count = loc1.groupby(['Sentiment', 'Location'])['Location'].count().reset_index(name='Cities')\n",
        "\n",
        "# Get the top 50 locations with the highest tweet counts\n",
        "location_count = location_count.nlargest(50, 'Cities')\n",
        "\n",
        "# Create the plot using Plotly Express\n",
        "# Set the x-axis as 'Cities' (tweet counts), y-axis as 'Location' (location names), and color based on 'Sentiment'\n",
        "fig = px.bar(location_count, x='Cities', y='Location', color='Sentiment')\n",
        "\n",
        "# Set the y-axis category order to be sorted by total ascending\n",
        "# This will sort the locations on the y-axis based on the total count of tweets\n",
        "fig.update_layout(yaxis={'categoryorder': 'total ascending'})\n",
        "\n",
        "# Set the figure size to 1400x600 pixels\n",
        "fig.update_layout(width=1400, height=600)\n",
        "\n",
        "# Add title to the graph\n",
        "fig.update_layout(title_text='Top 25 Locations with the Highest Tweet Counts by Sentiment')\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen chart, a Plotly Express bar plot, effectively visualizes the top 50 locations with the highest tweet counts, grouped by sentiment. Its suitability arises from:\n",
        "\n",
        "1.Effective representation of categorical data ('Location') against a numerical value ('Cities').\n",
        "\n",
        "2.Facilitating comparison of tweet counts across different locations through the height of bars.\n",
        "\n",
        "3.Top-N analysis, efficiently presenting the locations with the most significant tweet impact.\n",
        "\n",
        "4.Color representation to distinguish positive and negative tweet counts for each location.\n",
        "\n",
        "5.Interactivity with hover tooltips, zoom, and pan, enabling easy data exploration.\n",
        "\n",
        "The chart concisely communicates key insights about locations and their varying sentiments, aiding the understanding of the most influential locations in terms of tweet activity. Its visual clarity and interactive features make it useful for data exploration and analysis.\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data analysis indicates that London has the highest tweet volume, followed by the United States, while global tweets are the least frequent. Additionally, London stands out as the location with the most intense emotional expression, as it generated the highest number of both positive and negative tweets compared to other locations. These insights provide valuable information on tweet distribution and emotional sentiment, potentially aiding in understanding user behavior and sentiment patterns across different locations. Further analysis and context would be required to fully interpret the reasons behind these trends and their potential implications for businesses or social trends."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can potentially help create a positive business impact in the following ways:\n",
        "\n",
        "Marketing Strategy: Knowing that London has the highest tweet volume, businesses can focus their marketing efforts on this location to target a larger audience and potentially increase brand visibility and customer engagement.\n",
        "\n",
        "Customer Engagement: Understanding the emotional intensity in London's tweets can guide businesses in tailoring their customer engagement strategies to address the specific sentiments expressed by users in that location. This personalized approach can foster stronger customer relationships.\n",
        "\n",
        "Sentiment Analysis: By analyzing the positive and negative tweets, businesses can gauge customer sentiment towards their products or services and identify areas for improvement or areas of strength.\n",
        "\n",
        "Regarding potential negative growth insights, the data alone does not directly indicate negative trends. Further analysis, including sentiment analysis on a larger scale and consideration of external factors, would be required to identify any trends that could lead to negative growth.\n",
        "\n",
        "Negative growth insights may arise if:\n",
        "\n",
        "High Negative Sentiment: A significant proportion of negative tweets in London or any other location could indicate dissatisfaction with specific products, services, or brand experiences. If not addressed, it could impact customer loyalty and result in negative growth.\n",
        "\n",
        "Decreased Tweet Volume: If the tweet volume in a location declines over time, it might suggest waning interest or engagement with the brand or industry, leading to negative growth.\n",
        "\n",
        "Competitor Analysis: Comparing tweet trends with those of competitors can reveal any negative shifts in sentiment that might affect market share and growth.\n",
        "\n",
        "To draw meaningful conclusions about business impact and potential negative growth, combining sentiment analysis with other relevant metrics and understanding the broader context is essential. This comprehensive approach will aid businesses in making informed decisions to enhance positive impact and address any potential negative implications."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13- Bar Plot - Tweet Count by Week Day and Sentiment"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Define the order of hue categories for consistent plotting\n",
        "hue_order = ['Positive', 'Neutral', 'Negative']\n",
        "\n",
        "# Set a custom color palette for the plot\n",
        "custom_palette = {'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'}\n",
        "\n",
        "# Group by 'week day' and 'Sentiment' to get the tweet counts for each combination\n",
        "tweet_counts = tweet_df.groupby(['week day', 'Sentiment']).size().reset_index(name='Count')\n",
        "\n",
        "# Create the plot using Plotly Express\n",
        "fig = px.bar(tweet_counts, x='week day', y='Count', color='Sentiment', category_orders={'Sentiment': hue_order},\n",
        "             color_discrete_map=custom_palette)\n",
        "\n",
        "# Set axis labels and title\n",
        "fig.update_layout(xaxis_title='Week Day', yaxis_title='Count', title='Tweet Count by Week Day and Sentiment')\n",
        "\n",
        "# Show the legend outside the plot\n",
        "fig.update_layout(legend=dict(title='Sentiment'), legend_traceorder='normal', legend_title_font=dict(size=14))\n",
        "\n",
        "# Set the size of the plot\n",
        "fig.update_layout(width=1000, height=600)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen chart, a bar graph created using Plotly Express, presents a comprehensive visualization of tweet counts categorized by both week day and sentiment. The chart effectively communicates the distribution of positive, neutral, and negative tweets across different days of the week. The use of distinct colors for each sentiment category aids quick identification and understanding of the data. Plotly Express offers interactive features, enabling users to interact with the graph and explore specific tweet counts by hovering over the bars. Moreover, the chart's customization options, such as axis labels, title, and plot size, enhance the visual appeal and clarity of the presentation. Overall, this bar graph is a valuable tool for gaining insights into the tweet trends based on sentiment and week day, fostering a deeper understanding of user behavior and engagement patterns on the platform."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart revealed unexpected trends in Twitter activity, with Wednesday having the highest tweet volume, followed by Tuesday and Thursday, while Sunday exhibited the lowest activity. Contrary to the initial assumption of Sunday being the peak day, the middle of the week showed the most engagement. These insights suggest a potential influence of external factors, such as lockdown measures, on users' tweeting behavior. Further analysis may be needed to understand the reasons behind the observed patterns and their implications for businesses or social trends. Overall, the chart provides valuable information on weekly tweet distribution and user engagement trends."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can potentially help create a positive business impact in the following ways:\n",
        "\n",
        "Optimizing Marketing Strategy: Understanding the peak tweeting days (Wednesday, Tuesday, and Thursday) can help businesses schedule their social media campaigns and content to reach a larger audience during times of higher engagement.\n",
        "\n",
        "Customer Engagement: By focusing on days with higher tweet activity, businesses can engage with users when they are most active and responsive, leading to increased interactions and potential growth in brand awareness and customer loyalty.\n",
        "\n",
        "Identifying Potential Trends: Recognizing the influence of external factors, such as lockdown measures, on tweeting behavior can provide businesses with valuable context to adapt their strategies accordingly and align their offerings with current consumer sentiments.\n",
        "\n",
        "Regarding insights leading to negative growth, the data alone does not directly indicate negative trends. However, further analysis and monitoring of sentiment trends over time may uncover potential negative growth indicators. For example:\n",
        "\n",
        "Decreased Engagement: If tweet activity significantly declines across all days, it may suggest waning interest or disengagement with the platform, which could lead to negative growth in terms of user interaction and brand reach.\n",
        "\n",
        "Negative Sentiment Prevalence: A rising trend in negative sentiment on certain days or across all days could be detrimental to a business's reputation and result in negative growth if not addressed effectively.\n",
        "\n",
        "To fully assess the business impact and identify potential negative trends, combining sentiment analysis with other relevant metrics and market analysis is essential. This comprehensive approach will help businesses make data-driven decisions to capitalize on positive trends and mitigate any negative implications."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "tweet_df[\"Number Of Words\"] = tweet_df[\"OriginalTweet\"].str.split().apply(len)\n",
        "tweet_df = tweet_df.drop(columns=['month'])"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "correations=tweet_df.corr()\n",
        "print(correations)"
      ],
      "metadata": {
        "id": "ucO7jWrqSlw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix for the DataFrame 'tweet_df'\n",
        "correlations = tweet_df.corr()\n",
        "\n",
        "# Create a heatmap visualization of the correlation matrix\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Use absolute values for correlation to visualize the strength regardless of direction\n",
        "heatmap = sns.heatmap(abs(correlations), annot=True, cmap='crest')\n",
        "\n",
        "# Rotate the x-axis tick labels to 45 degrees\n",
        "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, ha='right')\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f8iwAMm4S5o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation chart, also known as a correlation matrix, is used to examine the relationship between two pairs of variables in a dataset. It employs a statistical measure called correlation, which indicates the strength and direction of the relationship between the variables. Correlation values range from -1 to 1, where 1 signifies a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 denotes no correlation between the variables. By analyzing the correlation matrix, researchers can gain insights into how closely the variables are related, helping to identify potential patterns and dependencies in the data. This tool is particularly valuable in understanding the associations between different variables and their impact on the dataset's overall behavior."
      ],
      "metadata": {
        "id": "xQj27X_bS3zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the correlation matrix, UserName and ScreenName exhibit a perfect correlation, indicating that they measure the same attribute or are highly related. On the other hand, Month and user names/screen names show a moderately favorable correlation, suggesting a potential relationship between them, but the direction of this relationship is unclear. The correlation analysis highlights the strong association between UserName and ScreenName, implying that they likely represent similar information, while the moderate correlation between Month and user names/screen names suggests a potential link, though the specific nature of this relationship requires further investigation."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(tweet_df,kind=\"scatter\")\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot is a valuable tool for examining the relationship between two variables and identifying the most informative features to explain this relationship. We have utilized the pair plot to gain insights into the associations between different variables and assess their relevance in explaining the observed patterns. By visualizing multiple variable combinations in a single chart, the pair plot aids in understanding the interconnections and dependencies within the dataset, providing valuable information for further analysis and modeling. The use of the pair plot is justified as it allows us to comprehensively explore the relationships between variables and make informed decisions in understanding the underlying data structure."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code generates a pair plot representing the absolute correlation matrix for the tweets dataset. Each pair of variables in the dataset is visualized through a grid of scatter plots. This pair plot allows us to explore and understand the relationships between different variables in the tweets dataset. By examining these visualizations, we can gain insights into the connections and dependencies among the variables, helping us comprehend the underlying data structure and identify potential patterns or trends. The pair plot serves as a powerful exploratory tool to visually assess the associations between variables and aids in making informed data-driven decisions during the analysis process."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We propose that the number of tweets related to COVID-19 experienced a significant decrease from March to April 2020. To validate this hypothesis, we will conduct a t-test, comparing the mean number of tweets in March with the mean number of tweets in April 2020."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis:** There is no significant difference in the mean number of tweets related to COVID-19 between March and April 2020.\n",
        "\n",
        "**Alternative Hypothesis:** The mean number of tweets related to COVID-19 in March is significantly higher than the mean number of tweets in April."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Access the 'TweetAt' column in the DataFrame 'tweet_df'\n",
        "tweet_at_column = tweet_df['TweetAt']\n",
        "tweet_at_column"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Define a function to extract tweets from a specific month\n",
        "def extractMonthTweets(item, month):\n",
        "    return item.month == month\n",
        "\n",
        "# Filter tweets from March and April using the 'extractMonthTweets' function and the 'apply' method\n",
        "march_tweets = tweet_df[tweet_df['TweetAt'].apply(lambda x: extractMonthTweets(x, 3))]\n",
        "april_tweets = tweet_df[tweet_df['TweetAt'].apply(lambda x: extractMonthTweets(x, 4))]\n",
        "\n",
        "# Count the number of tweets for each day in March and April\n",
        "march_tweets_count = march_tweets['TweetAt'].value_counts()\n",
        "april_tweets_count = april_tweets['TweetAt'].value_counts()\n",
        "\n",
        "# Print the mean and standard deviation of the tweet counts for March and April\n",
        "print('Mean of March tweets:', np.mean(march_tweets_count))\n",
        "print('Std of March tweets:', np.std(march_tweets_count))\n",
        "print('Mean of April tweets:', np.mean(april_tweets_count))\n",
        "print('Std of April tweets:', np.std(april_tweets_count))\n",
        "\n",
        "# Calculate the variables required for the z-test (two-sample test for means)\n",
        "mean1 = np.mean(march_tweets_count)\n",
        "mean2 = np.mean(april_tweets_count)\n",
        "std1 = np.std(march_tweets_count)\n",
        "std2 = np.std(april_tweets_count)\n",
        "n1 = march_tweets_count.shape[0]\n",
        "n2 = april_tweets_count.shape[0]\n",
        "\n",
        "# Calculate the z-value for the two-sample z-test\n",
        "zValue = ((mean1 - mean2) - 0) / np.sqrt(((std1**2) / n1) + (std2**2 / n2))\n",
        "print('zValue:', zValue)"
      ],
      "metadata": {
        "id": "rmhDYtsTmNAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to find the region to the left of the z-score calculated previously.\n",
        "p = stats.norm.cdf(zValue)\n",
        "\n",
        "# Print the calculated P-value\n",
        "print(f'The P value is {p}')\n",
        "\n",
        "# Check the significance level (commonly 0.05) to determine whether to accept or reject the null hypothesis\n",
        "if p > 0.05:\n",
        "    print('Null Hypothesis accepted. Mean of March tweets >= Mean of April tweets')\n",
        "else:\n",
        "    print('Rejected Null Hypothesis. Mean of March tweets < Mean of April tweets')"
      ],
      "metadata": {
        "id": "33FmaYqOmUY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed a t-test to compare the average number of tweets in March with April. The t-test determines if there is a statistically significant difference between the means of the two groups. This analysis helps validate whether there was a notable change in COVID-19-related tweets between the two months."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "A t-test is a statistical test used to evaluate whether there is a significant difference between the means of two groups. It specifically examines if the observed difference in means is statistically significant or simply due to random variation."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no significant difference in the number of tweets related to COVID-19 between weekdays and weekends.\n",
        "\n",
        "To test this hypothesis, we can perform a two-sample t-test comparing the mean number of tweets on weekdays to the mean number of tweets on weekends."
      ],
      "metadata": {
        "id": "wpig6XYYnGUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis:** There is no significant difference in the mean number of tweets related to COVID-19 between weekdays and weekends.\n",
        "\n",
        "**Alternative Hypothesis:** The mean number of tweets related to COVID-19 on weekends is significantly different from the mean number of tweets on weekdays."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Perform Statistical Test and obtain P-Value\n",
        "# If necessary, convert the Tweet_Date column to datetime format.\n",
        "tweet_df['TweetAt'] = pd.to_datetime(tweet_df['TweetAt'])\n",
        "\n",
        "# Create a new column to represent the numerical day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\n",
        "tweet_df['DayOftheWeek'] = tweet_df['TweetAt'].dt.weekday\n",
        "\n",
        "# Filter weekdays and weekends using the new 'DayOftheWeek' column\n",
        "# Weekdays are represented by integers 0 to 4 (Monday to Friday)\n",
        "# Weekends are represented by integers 5 and 6 (Saturday and Sunday)\n",
        "weekday = tweet_df[tweet_df['DayOftheWeek'] < 5]['DayOftheWeek']\n",
        "weekend = tweet_df[tweet_df['DayOftheWeek'] >= 5]['DayOftheWeek']\n",
        "\n",
        "# Perform a t-test on the 'DayOftheWeek' columns to compare weekdays and weekends\n",
        "t_stat, p_val = stats.ttest_ind(weekday, weekend)\n",
        "\n",
        "# Print the t-statistic and p-value\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(f\"p-value: {p_val:.20f}\")\n",
        "\n",
        "# Check the significance level (commonly 0.05) to determine whether to accept or reject the null hypothesis\n",
        "if p_val > 0.05:\n",
        "    print('Null Hypothesis accepted')\n",
        "else:\n",
        "    print('Rejected Null Hypothesis')\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test performed to obtain the P-value is the independent two-sample t-test (also known as the Student's t-test). This test is used to compare the means of two independent groups and determine if there is a statistically significant difference between them. In this case, the two groups being compared are the weekdays and weekends, and the test is conducted on the numerical day of the week data. The null hypothesis assumes that there is no significant difference in the means of the two groups, while the alternative hypothesis suggests that there is a significant difference. The obtained P-value helps determine whether the null hypothesis should be accepted or rejected based on the chosen significance level (commonly 0.05)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The independent two-sample t-test was chosen because it is suitable for comparing the means of two independent groups (weekdays and weekends) with continuous data (day of the week). It allows us to assess whether the observed difference in tweet counts between weekdays and weekends is statistically significant. The test assumes normal distribution and independence of samples, which are applicable in this scenario. The obtained P-value helps determine whether to accept or reject the null hypothesis based on a chosen significance level (e.g., 0.05). The test enables us to draw conclusions about the significance of the difference in tweet counts between weekdays and weekends."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We propose that the number of tweets related to COVID-19 differs significantly between locations in India and the rest of the locations. To verify this hypothesis, we will conduct a two-sample t-test, comparing the mean number of tweets from Indian locations with the mean number of tweets from all other locations."
      ],
      "metadata": {
        "id": "mTV-fncmVoh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis:** There is no significant difference in the mean number of tweets related to COVID-19 between Indian locations and the rest.\n",
        "\n",
        "**Alternative Hypothesis:** There is a significant difference in the mean number of tweets related to COVID-19 between Indian locations and the rest of the locations."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform a custom filtering to extract tweets related to Indian and Non-Indian topics\n",
        "\n",
        "# Define a function to extract tweets related to Indian topics\n",
        "def extractTweets(item, isIndian):\n",
        "    if isIndian:\n",
        "        # Return True for tweets containing 'india' (case-insensitive) but not 'indiana'\n",
        "        return ('india' in item.lower() and 'indiana' not in item.lower())\n",
        "    else:\n",
        "        # Return True for tweets not containing 'india' (case-insensitive) or containing 'indiana'\n",
        "        return ('india' not in item.lower() or 'indiana' in item.lower())\n",
        "\n",
        "# Filter tweets related to Indian topics and Non-Indian topics using the 'extractTweets' function and the 'apply' method\n",
        "IndianTweets = tweet_df[tweet_df['OriginalTweet'].apply(lambda x: extractTweets(x, True))]\n",
        "NonIndianTweets = tweet_df[tweet_df['OriginalTweet'].apply(lambda x: extractTweets(x, False))]\n",
        "\n",
        "# Count the number of tweets for each day in the Indian and Non-Indian tweet subsets\n",
        "Indian_tweets_count = IndianTweets['TweetAt'].value_counts()\n",
        "NonIndian_tweets_count = NonIndianTweets['TweetAt'].value_counts()\n",
        "\n",
        "# Print the mean and standard deviation of the tweet counts for Indian and Non-Indian tweets\n",
        "print('Mean of Indian tweets:', np.mean(Indian_tweets_count))\n",
        "print('Std of Indian tweets:', np.std(Indian_tweets_count))\n",
        "print('Mean of Non-Indian tweets:', np.mean(NonIndian_tweets_count))\n",
        "print('Std of Non-Indian tweets:', np.std(NonIndian_tweets_count))\n",
        "\n",
        "# Calculate the variables required for the z-test (two-sample test for means)\n",
        "mean1 = np.mean(Indian_tweets_count)\n",
        "mean2 = np.mean(NonIndian_tweets_count)\n",
        "std1 = np.std(Indian_tweets_count)\n",
        "std2 = np.std(NonIndian_tweets_count)\n",
        "n1 = Indian_tweets_count.shape[0]\n",
        "n2 = NonIndian_tweets_count.shape[0]\n",
        "\n",
        "# Calculate the z-value for the two-sample z-test\n",
        "zValue = ((mean1 - mean2) - 0) / np.sqrt(((std1**2) / n1) + (std2**2 / n2))\n",
        "print('zValue:', zValue)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the z-score of interest\n",
        "\n",
        "# Lookup the area to the left of the z-score using the CDF function of the standard normal distribution\n",
        "# Calculate the p-value using the CDF (Cumulative Distribution Function) of the standard normal distribution\n",
        "p = stats.norm.cdf(zValue)\n",
        "\n",
        "# Print the p-value with 20 decimal places for precision\n",
        "print(f\"p-value: {p:.20f}\")\n",
        "\n",
        "# Check the significance level (commonly 0.05) to determine whether to accept or reject the null hypothesis\n",
        "if p > 0.05:\n",
        "    print('Null Hypothesis accepted. Mean of Indian Tweets >= Mean of Non-Indian tweets')\n",
        "else:\n",
        "    print('Rejected Null Hypothesis. Mean of Indian Tweets < Mean of Non-Indian tweets')\n"
      ],
      "metadata": {
        "id": "UtNkoUWSWVo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test this hypothesis, we can perform a two-sample t-test comparing the mean number of tweets in India and the mean number of tweets in the Non-Indian countries."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate whether there is a significant difference between the means of two groups, a t-test is a statistical test that is employed. In particular, it examines whether the difference in means between the two groups is statistically significant or more likely the result of chance."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "tweet_df.isnull().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset, the 'Location' column contains 8590 null values. Typically, in sentiment analysis tasks using machine learning models, the 'Location' column is not considered as a relevant feature. Therefore, the presence of missing values in this column is not likely to significantly impact the performance or outcome of the sentiment analysis model."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "print(tweet_df[['OriginalTweet', 'Sentiment']].info())\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers were not treated or handled in our analysis because we did not use any column with continuous numerical data as a dependent or independent variable during the model training process. As a result, there was no need to address outliers since our analysis focused on other aspects of the dataset."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Import the nltk library, which provides natural language processing tools\n",
        "import nltk\n",
        "\n",
        "# Import the stopwords corpus from nltk, which contains common words that may not contribute much to the analysis\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Create a list called 'stop' containing the English stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the PorterStemmer class from the nltk.stem.porter module\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Create an instance of the PorterStemmer class, which is used for word stemming\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "uhoYvuB-JMtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Remove Punctuations\n",
        "# Remove URLs & Remove words and digits contain digits\n",
        "# Remove Stopwords\n",
        "# Remove White spaces\n",
        "# Rephrase Text\n",
        "# Tokenization\n",
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "def transform_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Tokenize text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    words = [word for word in words if word.isalnum()]\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    punctuation_set = set(string.punctuation)\n",
        "    words = [word for word in words if word not in stopwords_set and word not in punctuation_set]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join words into a string and return\n",
        "    return ' '.join(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "DUuygetcgi9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use only Lemmatization normalization technique because Lemmatization is a technique used for text normalization that reduces words to their base or dictionary form, known as a lemma .Hence ,we used Lemmatization technique as proper text form and later used for classfication modelling .\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "transform_text(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today https/20.89.\")"
      ],
      "metadata": {
        "id": "zZWGjbaUoUs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the `transform_text()` function to each value in the 'OriginalTweet' column of `tweet`\n",
        "tweet_df[\"Clean_Tweets\"] = tweet_df['OriginalTweet'].apply(transform_text)"
      ],
      "metadata": {
        "id": "0kLAcQfYt_z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the 'OriginalTweet' value in the row with integer in `tweet`\n",
        "tweet_df.iloc[25160][\"OriginalTweet\"]"
      ],
      "metadata": {
        "id": "yHJPsa4xuIXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "fkxn2LcmuOB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column in `tweet` called 'temp_list'\n",
        "tweet_df['temp_list'] = tweet_df['Clean_Tweets'].apply(lambda x:str(x).split())"
      ],
      "metadata": {
        "id": "ohka06dRuWOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df['temp_list']"
      ],
      "metadata": {
        "id": "A33L9RW6ucxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Create a flattened list of all words in the nested list column\n",
        "word_list = [word for sublist in tweet_df['temp_list'] for word in sublist]\n",
        "\n",
        "# Count the frequency of each word and store in a Counter object\n",
        "word_counts = Counter(word_list)\n",
        "\n",
        "# Create a dataframe of the top 30 most common words\n",
        "top_words = pd.DataFrame(word_counts.most_common(30), columns=['Common_words', 'count'])\n",
        "\n",
        "# Apply a background gradient to the dataframe for better visualization\n",
        "styled_top_words = top_words.style.background_gradient(cmap='Reds')"
      ],
      "metadata": {
        "id": "n33pSJaWyoD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words"
      ],
      "metadata": {
        "id": "tbxmfeC5y0EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame called `neutral` containing only the 'Clean_Tweets' column from `tweet` where the 'Sentiment' column is 'Neutral','positive','negative'.\n",
        "neutral=pd.DataFrame(tweet_df[['Clean_Tweets']] [tweet_df['Sentiment'] == 'Neutral'])\n",
        "positive=pd.DataFrame(tweet_df[['Clean_Tweets']] [tweet_df['Sentiment'] == 'Positive'])\n",
        "negative=pd.DataFrame(tweet_df[['Clean_Tweets']] [tweet_df['Sentiment'] == 'Negative'])"
      ],
      "metadata": {
        "id": "pptMeSq_y7Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive\n"
      ],
      "metadata": {
        "id": "NLpgKxRwy-Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative"
      ],
      "metadata": {
        "id": "gyxg0Iq6ikBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neutral"
      ],
      "metadata": {
        "id": "lNVf09kCzD7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud"
      ],
      "metadata": {
        "id": "qNz3vHJ1zHf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list called `spam_corpus`\n",
        "spam_corpus = []\n",
        "# Iterate over each row in a DataFrame called `tweet` where the 'Sentiment' column has the value 'Neutral'\n",
        "for msg in tweet_df[tweet_df['Sentiment'] =='Neutral']['Clean_Tweets'].tolist():\n",
        "  # Split the 'Clean_Tweets' value into individual words using the `split()` method\n",
        "    for word in msg.split():\n",
        "       # Append each word to the `spam_corpus` list\n",
        "        spam_corpus.append(word)"
      ],
      "metadata": {
        "id": "zAT2zbM5zN9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words"
      ],
      "metadata": {
        "id": "bWTGEG73izUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the `Counter` class from the `collections` module\n",
        "from collections import Counter\n",
        "plt.figure(figsize=(20,10))\n",
        "# Import the `seaborn` library for creating data visualizations\n",
        "sns.barplot(x='Common_words',y='count',data=top_words)\n",
        "# Set the x-axis labels to be vertical\n",
        "plt.xticks(rotation='vertical')\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TNj3rBf-i43B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the `WordCloud` class from the `wordcloud` module\n",
        "from wordcloud import WordCloud\n",
        "wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')"
      ],
      "metadata": {
        "id": "0Vy3XZHXi_1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a word cloud using the `WordCloud` object and the text data from the 'Clean_Tweets' column of a DataFrame called `neutral`\n",
        "tweet_wc = wc.generate(str(neutral['Clean_Tweets']))\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(tweet_wc)"
      ],
      "metadata": {
        "id": "jmZPXPFGjDrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a word cloud using the `WordCloud` object and the text data from the 'Clean_Tweets' column of a DataFrame called `positive`\n",
        "tweet_wc = wc.generate(str(positive['Clean_Tweets']))\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(tweet_wc)"
      ],
      "metadata": {
        "id": "mmOW3zFFjJMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a word cloud using the `WordCloud` object and the text data from the 'Clean_Tweets' column of a DataFrame called `negative`\n",
        "tweet_wc = wc.generate(str(negative['Clean_Tweets']))\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(tweet_wc)"
      ],
      "metadata": {
        "id": "vgcT7rTijPFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X= tweet_df['Clean_Tweets']\n",
        "y= tweet_df['Sentiment']"
      ],
      "metadata": {
        "id": "B9GoryIIgXbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Train test split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=10)\n"
      ],
      "metadata": {
        "id": "C9X-perxhoUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting ratio is 80% used for Training Data and 20% used for Test Data. By providing more data for training, the model can learn more patterns and trends in the data, which can lead to better predictions on new or unseen data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking shape of splitted data\n",
        "print(X_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "oUU_p66vh51U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6. Data Scaling**"
      ],
      "metadata": {
        "id": "HDVo6wg1j89X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "DYvOjXXMkJEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here no need of any scaling of Dataset becuase here only check sentiment of people,hence we analyze only User sentiment tweet according to the secenario of Covid-19."
      ],
      "metadata": {
        "id": "_gcwKlcDkKhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display information about the DataFrame 'tweet_df' to understand its structure and check for missing values.\n",
        "tweet_df.info()"
      ],
      "metadata": {
        "id": "wqp40IFWielb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No , here we decided that We used only two column for machine learning modelling i.e 'Original Tweet' & 'Sentiment' and in both column eqaul no of rows"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Count Vectorization (Bag of words)**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count vectorization is a process of converting a piece of text into a numerical format that can be used by machine learning algorithms. In this process, the text is first split into words or tokens, and then each token is counted to create a vector of numbers representing the frequency of each word in the text."
      ],
      "metadata": {
        "id": "UGCxbbO2KBP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of words\n",
        "bw=CountVectorizer(binary=False,max_df=1.0,min_df=5,ngram_range=(1,2))\n",
        "bw_X_train=bw.fit_transform(X_train.astype(str).str.strip())\n"
      ],
      "metadata": {
        "id": "YwroXvXSKgBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape of the NumPy array bw_X_train\n",
        "bw_X_train.shape"
      ],
      "metadata": {
        "id": "p-dfUXzmKgzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Trained CountVectorizer 'bw' to transform the test data 'X_test' into a bag-of-words representation\n",
        "# The text data in 'X_test' is first converted to a string representation & then stripped of leading & trailing whitespace characters\n",
        "bw_X_test=bw.transform(X_test.astype(str).str.strip())\n"
      ],
      "metadata": {
        "id": "d7i6XivOKl00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get the shape of the NumPy array bw_X_test\n",
        "bw_X_test.shape"
      ],
      "metadata": {
        "id": "h5h74oLxKqfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ML Model - 1 Logistic regression with GridSearchCV**"
      ],
      "metadata": {
        "id": "qyPYgGq7K0tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a statistical model used to predict the probability of an event occurring. It is particularly well-suited for binary classification problems, where the outcome can be either 0 or 1 (e.g., yes/no, true/false). The model estimates the probability of the event happening by transforming the odds into a range between 0 and 1.\n",
        "\n",
        "In multiclass classification, Logistic Regression can also be extended to handle scenarios where the outcome can have more than two classes. In this case, the model combines the linear relationship of one or more independent variables to predict the probabilities of multiple classes, assigning the class with the highest probability as the final prediction."
      ],
      "metadata": {
        "id": "7x_oLZboK9LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the logistic regression model\n",
        "lr_cv = LogisticRegression()\n",
        "\n",
        "# Defining a dictionary of hyperparameters to tune over\n",
        "parameters = {\n",
        "    'penalty': ['l1', 'l2'],  # Regularization penalty to apply (L1 or L2)\n",
        "    'C': [100, 10, 1.0, 0.1, 0.01]  # Inverse of regularization strength (lower values indicate stronger regularization)\n",
        "}\n",
        "\n",
        "# Creating a GridSearchCV object with cross-validation of 15\n",
        "logreg_Gcv = GridSearchCV(lr_cv, parameters, cv=15)\n",
        "\n",
        "# Fitting the training data to the GridSearchCV object to find the best hyperparameters\n",
        "logreg_Gcv.fit(bw_X_train, y_train)"
      ],
      "metadata": {
        "id": "jrFe0zdpLCG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the target values using the Logistic Regression model with GridSearchCV\n",
        "# The 'logreg_Gcv' model has already been trained using GridSearchCV\n",
        "# 'bw_X_test' contains the feature data of the test set, which has been preprocessed and transformed\n",
        "# The predicted values for the target variable are stored in 'pred_lr_cv'\n",
        "pred_lr_cv = logreg_Gcv.predict(bw_X_test)\n"
      ],
      "metadata": {
        "id": "TC7IKlDFLFMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lr_cv"
      ],
      "metadata": {
        "id": "Uqt3mSi7LI5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the Logistic Regression model with GridSearchCV\n",
        "# 'y_test' contains the true target values for the test set\n",
        "# 'pred_lr_cv' contains the predicted values for the target variable from the model\n",
        "accuracy_lr_cv = accuracy_score(y_test, pred_lr_cv)\n",
        "\n",
        "# Print the calculated accuracy\n",
        "print(\"Accuracy:\", accuracy_lr_cv)\n"
      ],
      "metadata": {
        "id": "twVClnUdLNyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the labels for the target classes\n",
        "label = ['neutral', 'positive', 'negative']\n",
        "\n",
        "# Generate the classification report to evaluate performance metrics\n",
        "# 'y_test' contains the true target values for the test set\n",
        "# 'pred_lr_cv' contains the predicted values for the target variable from the model\n",
        "report = classification_report(y_test, pred_lr_cv, target_names=label)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "PzEnnP-NLQ8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix for the Logistic Regression model with GridSearchCV\n",
        "cf1 = confusion_matrix(y_test, pred_lr_cv)\n",
        "\n",
        "# Create a heatmap to visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = plt.subplot()\n",
        "\n",
        "# Define the color map for the heatmap (e.g., 'Blues', 'Reds', 'Greens', 'Oranges', etc.)\n",
        "cmap = 'Blues'\n",
        "\n",
        "# Use seaborn's heatmap function to plot the confusion matrix with annotations\n",
        "sns.heatmap(cf1, annot=True, fmt=\".0f\", ax=ax, cmap=cmap, linewidths=1, linecolor='gray', annot_kws={\"size\": 15})\n",
        "\n",
        "# Set labels, title, and ticks for better understanding\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Logistic Regression with CV)', fontsize=20)\n",
        "\n",
        "# Set tick labels for both x-axis and y-axis to display class labels properly\n",
        "ax.xaxis.set_ticklabels(labels, fontsize=12)\n",
        "ax.yaxis.set_ticklabels(labels, fontsize=12)\n",
        "\n",
        "# Adjust the color bar properties (if needed)\n",
        "cbar = ax.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=12)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Kw4sdALuLUqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we employed the Logistic Regression algorithm to classify COVID-19 tweets based on their sentiment, achieving an accuracy of 79%. By using this algorithm, we were able to improve various evaluation metrics scores for different sentiment labels, including 'Negative', 'Neutral', and 'Positive'. Notably, precision, recall, and F1 score, which are essential measures for assessing model performance, showed improvement with the use of Logistic Regression. This indicates that the model's ability to correctly classify tweets into their respective sentiment categories was enhanced, leading to more reliable predictions."
      ],
      "metadata": {
        "id": "bLp8aE3gLcDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we utilized GridSearchCV to fine-tune the hyperparameters of the Logistic Regression model. The two hyperparameters that were tuned are the regularization penalty, which can be either L1 or L2, and the inverse of regularization strength (C). GridSearchCV performs an exhaustive search over a specified set of hyperparameter values to find the combination that results in the best performance.\n",
        "\n",
        "To evaluate the model's performance, we employed cross-validation with a cv parameter set to 15. Cross-validation helps us to obtain a more robust estimate of the model's performance by splitting the data into multiple subsets (folds) and iteratively training and testing the model on different combinations of these subsets. This way, we get a more reliable assessment of how well the model generalizes to unseen data."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Evaluation metric Score Chart shows the performance results of the model:\n",
        "\n",
        "**Accuracy:** The model achieved an accuracy of 79%, indicating the percentage of correctly classified tweets out of the total.\n",
        "\n",
        "**Precision:** The precision score is 77%, representing the proportion of correctly predicted positive sentiment tweets among all predicted positive sentiment tweets.\n",
        "\n",
        "**Recall:** The recall score is also 77%, which denotes the proportion of correctly identified positive sentiment tweets out of all actual positive sentiment tweets.\n",
        "\n",
        "**F1-score:** The F1-score is 77%, which is the harmonic mean of precision and recall. It is used to balance precision and recall, providing a single metric to assess the model's overall performance.\n",
        "\n",
        "These scores indicate that the model has performed well in classifying COVID-19 tweets based on their sentiment, with a balanced trade-off between correctly identifying positive sentiment tweets and minimizing false positives and false negatives. Overall, the model shows promising results in sentiment classification for the given dataset."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 2 - Decision Tree Classifier with GridSearchCV**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision tree is a hierarchical structure resembling a flowchart. Each internal node of the tree represents a test on a specific attribute or feature from the data. The branches emanating from each node represent the possible outcomes of the test. Based on these outcomes, the algorithm progresses down the tree until reaching a leaf node, which represents a class label or a numerical value (in the case of regression). In essence, a Decision tree uses a series of tests and their outcomes to make decisions and classify data points into different categories or predict numerical values."
      ],
      "metadata": {
        "id": "kxmb9hQTL5OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Decision Tree model\n",
        "dt_cv = DecisionTreeClassifier()\n",
        "\n",
        "# Fitting the training data to the Decision Tree model\n",
        "dt_cv.fit(bw_X_train, y_train)\n",
        "\n",
        "# Making predictions on the test data using the fitted model\n",
        "pred_dt_cv = dt_cv.predict(bw_X_test)"
      ],
      "metadata": {
        "id": "WKXSM9R3L-nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cross-validated accuracy for the Decision Tree model\n",
        "# cv=5 indicates 5-fold cross-validation\n",
        "cv_score_dt_cv = cross_val_score(dt_cv, bw_X_train, y_train, cv=5)\n",
        "\n",
        "# Calculate and print the mean accuracy of the cross-validated scores\n",
        "# np.mean() computes the average of the cross-validated accuracy scores\n",
        "print(\"Accuracy: {}\".format(np.mean(cv_score_dt_cv)))"
      ],
      "metadata": {
        "id": "I0BWEUZ0MA3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the labels for the classification report\n",
        "label = ['Neutral', 'Positive', 'Negative']\n",
        "\n",
        "# Generate and print the classification report for the Decision Tree model\n",
        "# The classification report provides precision, recall, F1-score, and support for each class label.\n",
        "# y_test contains the actual labels, and pred_dt_cv contains the predicted labels.\n",
        "report_dt_cv = classification_report(y_test, pred_dt_cv, target_names=label)\n",
        "print(report_dt_cv)"
      ],
      "metadata": {
        "id": "Lp3JYZLXMGL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate the confusion matrix for the Decision Tree model\n",
        "cf2 = confusion_matrix(y_test, pred_dt_cv)\n",
        "\n",
        "# Create a heatmap to visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = plt.subplot()\n",
        "\n",
        "# Use seaborn's heatmap to plot the confusion matrix with annotations\n",
        "sns.heatmap(cf2, annot=True, fmt=\".0f\", ax=ax, cmap='YlGnBu')\n",
        "\n",
        "# Set the labels, title, and ticks for the plot\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Decision Tree with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, the Scikit-learn library is utilized to evaluate the performance of the Decision Tree model through cross-validation. Cross-validation is a technique used to assess how well a model generalizes to new data by dividing the dataset into multiple subsets, or folds. The model is trained on a subset of the data and tested on the remaining fold. This process is repeated several times, with each fold used as both training and testing data.\n",
        "\n",
        "In this case, 5-fold cross-validation is performed, which means the dataset is divided into 5 equal parts. The Decision Tree model is trained and evaluated 5 times, with each subset used as the testing data once and the remaining four subsets used as training data in each iteration.\n",
        "\n",
        "After completing the cross-validation, the np.mean function is applied to calculate the average accuracy score across all 5 folds. The accuracy score represents the proportion of correctly classified instances out of all instances in the testing data. By taking the mean of the accuracy scores from each fold, we obtain a more robust and reliable estimate of the model's overall performance.\n",
        "\n",
        "Using cross-validation with 5 folds helps ensure that the model's performance evaluation is less dependent on a specific split of the data. It provides a more representative assessment of the model's ability to generalize to unseen data and helps avoid potential issues with overfitting or underfitting."
      ],
      "metadata": {
        "id": "1b81WL_5MRMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the evaluation metric scores for the Decision Tree model:\n",
        "\n",
        "Accuracy: 70%\n",
        "\n",
        "Precision: 71%\n",
        "\n",
        "Recall: 70%\n",
        "\n",
        "F1-score: 71%\n",
        "\n",
        "These metrics offer valuable insights into the model's performance in classifying tweets based on sentiment. The accuracy score represents the proportion of correctly classified instances overall, while precision indicates how many of the predicted positive instances are correct. Recall, also known as sensitivity, shows how well the model identifies actual positive instances. The F1-score is a harmonic mean of precision and recall, providing a balanced assessment of the model's performance.\n",
        "\n",
        "These evaluation metrics help us understand how well the Decision Tree model generalizes to new and unseen data, and they guide us in assessing the model's effectiveness in sentiment classification of COVID-19 tweets."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, the Decision Tree algorithm was employed to classify COVID-19 tweets based on sentiment, achieving an accuracy of 70%. The model's evaluation metrics, such as precision, recall, and F1-score, were computed for each sentiment label ('Negative', 'Neutral', 'Positive'). Notably, the model demonstrated improved performance in distinguishing between different sentiment categories, as indicated by the increased precision, recall, and F1-score values. These metrics provide valuable insights into the model's ability to correctly identify positive instances, its sensitivity in detecting actual positive instances, and its overall effectiveness in sentiment classification of COVID-19 tweets."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 3 - KNN(K-Nearest Neighbours) with GridSearchCV**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Nearest Neighbors (KNN) algorithm is a straightforward and effective classification algorithm. It operates by locating the K nearest data points (instances) in the training dataset to a given input instance and then determining the majority class among those neighbors. The predicted label for the input instance is assigned as the class that appears most frequently among the K neighbors. KNN is a non-parametric and lazy learning algorithm, meaning it does not make any assumptions about the underlying data distribution and it defers training until it needs to make predictions.\n",
        "\n",
        "In more detail, when a new instance needs to be classified, KNN calculates the distance between the new instance and all instances in the training dataset. The distance metric used can vary, with common choices being Euclidean distance, Manhattan distance, or cosine similarity. The algorithm then selects the K closest instances based on the chosen distance metric. The majority class among these K neighbors is determined, and the new instance is assigned that class as its predicted label.\n",
        "\n",
        "KNN is often used for classification tasks but can also be adapted for regression problems. It is a flexible and versatile algorithm that can be applied to a wide range of datasets. However, it does have some drawbacks, including high computational cost when dealing with large datasets, sensitivity to irrelevant features, and the need to choose an appropriate value for K.\n",
        "\n",
        "Overall, KNN is a valuable tool in the field of machine learning, particularly when dealing with small to moderate-sized datasets and non-linear decision boundaries. Its simplicity and ease of implementation make it a popular choice for various classification tasks."
      ],
      "metadata": {
        "id": "BB4N4eF5M2Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize the KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define the parameter grid with a range of values for 'n_neighbors'\n",
        "param_grid = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
        "\n",
        "# Perform grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the KNN model to the training data and perform grid search\n",
        "grid_search.fit(bw_X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding mean cross-validation score\n",
        "print('Best hyperparameters:', grid_search.best_params_)\n",
        "print('Mean cross-validation score:', grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the values using the KNN model with the best hyperparameters obtained from grid search\n",
        "pred_knn_cv = grid_search.predict(bw_X_test)\n",
        "\n",
        "# Print the predicted values\n",
        "print(pred_knn_cv)"
      ],
      "metadata": {
        "id": "4rtXvxWpNAaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_knn_cv"
      ],
      "metadata": {
        "id": "cjOG22kRNBGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculating the accuracy of KNN model\n",
        "accuracy_KNN = accuracy_score(y_test, pred_knn_cv)\n",
        "\n",
        "# Displaying the accuracy\n",
        "print(\"Accuracy :\", accuracy_KNN)"
      ],
      "metadata": {
        "id": "VMK7IvkINEwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the labels for the classes (target values)\n",
        "label = ['Neutral', 'Positive', 'Negative']\n",
        "\n",
        "# Assuming you have defined 'y_test' and 'pred_knn_cv' as the true labels and predicted labels respectively,\n",
        "# you can use the 'classification_report' function to generate a classification report.\n",
        "# This report provides precision, recall, F1-score, and support for each class.\n",
        "report = classification_report(y_test, pred_knn_cv)\n",
        "\n",
        "# Now, let's print the classification report to see the performance metrics.\n",
        "print(report)"
      ],
      "metadata": {
        "id": "8KDtUC4QNIpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the confusion matrix.\n",
        "cf_knn = confusion_matrix(y_test, pred_knn_cv)\n",
        "\n",
        "# Define the labels for the classes (target values)\n",
        "labels = ['Neutral', 'Positive', 'Negative']\n",
        "\n",
        "# Set up the plot figure with the heatmap\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.heatmap(cf_knn, annot=True, fmt=\".0f\", cmap='Blues')  # Using the 'Blues' colormap for the heatmap\n",
        "\n",
        "# Set labels, title, and ticks for better visualization\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (KNN with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "\n",
        "# Ensure proper visualization of the plot\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z2O_zvj1NM50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data was classified using the K-Nearest Neighbors (KNN) algorithm, but the achieved accuracy was only 39%. Despite applying various evaluation metrics, including precision, recall, and F1 score, for each label ('Negative', 'Neutral', 'Positive'), the model's performance did not show any improvement compared to the baseline model.\n",
        "\n",
        "In other words, the KNN algorithm did not perform well in accurately classifying the data into the different classes, and the evaluation metrics indicated that the model's performance was no better than a basic, random classification approach. This suggests that either the KNN algorithm may not be suitable for this specific dataset, or further optimizations and feature engineering might be needed to improve the model's performance."
      ],
      "metadata": {
        "id": "s0yvUGYaNT0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The n_neighbors hyperparameter for the K-Nearest Neighbors (KNN) algorithm has a range of possible values defined in the param_grid dictionary. This dictionary contains a list of integers from 1 to 9, which will be used as the values of n_neighbors during hyperparameter tuning.\n",
        "\n",
        "To find the best value for n_neighbors, GridSearchCV is employed. GridSearchCV is a method that performs an exhaustive search over a specified hyperparameter grid, in this case, the param_grid. It evaluates the performance of the KNN algorithm using each value of n_neighbors through 5-fold cross-validation.\n",
        "\n",
        "In 5-fold cross-validation, the dataset is divided into 5 equal parts or \"folds.\" The model is trained on 4 of these folds and validated on the remaining 1 fold. This process is repeated 5 times, each time using a different fold for validation. The performance metrics for each combination of n_neighbors and fold are then averaged to provide a robust estimate of the model's performance for each n_neighbors value.\n",
        "\n",
        "By conducting this grid search with cross-validation, we can determine the optimal value for n_neighbors that results in the best performance for the KNN algorithm on the given dataset."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation metric scores for the K-Nearest Neighbors (KNN) algorithm are as follows:\n",
        "\n",
        "Accuracy: 38% Precision: 53% Recall: 47% F1-score: 40% These scores indicate the performance of the KNN algorithm on the given dataset. Let's elaborate on each metric:\n",
        "\n",
        "Accuracy: The accuracy is the percentage of correctly classified instances out of the total number of instances in the dataset. In this case, the KNN algorithm achieved an accuracy of 38%, which means that only 38% of the data points were correctly classified.\n",
        "\n",
        "Precision: Precision is the ratio of true positive predictions to the total number of positive predictions made by the model. It measures the ability of the model to avoid false positives. Here, the precision is 53%, indicating that 53% of the predicted positive instances were actually true positives.\n",
        "\n",
        "Recall: Recall, also known as sensitivity or true positive rate, is the ratio of true positive predictions to the total number of actual positive instances in the dataset. It measures the ability of the model to correctly identify positive instances. The recall value of 47% suggests that the model captured only 47% of the actual positive instances.\n",
        "\n",
        "F1-score: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure of the model's performance, especially when dealing with imbalanced datasets. The F1-score here is 40%, which indicates the overall trade-off between precision and recall.\n",
        "\n",
        "The relatively low values for these evaluation metrics suggest that the KNN algorithm's performance on the dataset is not satisfactory. It might require further analysis, hyperparameter tuning, or a different algorithm to improve its classification accuracy and overall predictive capability."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model 4 - Stochastic Gradient Descent with GridSearchCV**"
      ],
      "metadata": {
        "id": "_WgS5f3OT_oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Stochastic Gradient Descent (SGD), a more computationally efficient approach is employed compared to the traditional Gradient Descent algorithm. Instead of computing the gradient over the entire training set, SGD computes the gradient on a random subset (mini-batch) of the training data. This process significantly reduces the computational burden and allows for faster convergence during training.\n",
        "\n",
        "The key idea behind SGD is to update the model parameters after processing each mini-batch of data. This incremental updating enables the algorithm to make frequent adjustments to the model, which can lead to faster convergence compared to the traditional Gradient Descent approach, where updates occur after evaluating the entire training set.\n",
        "\n",
        "The process of updating parameters for each mini-batch is repeated until the model reaches convergence or a predefined maximum number of iterations is reached. The stochastic nature of SGD introduces a level of randomness, which can help the model escape local minima and potentially find better solutions in the parameter space.\n",
        "\n",
        "SGD is particularly advantageous when dealing with large datasets or high-dimensional data, as it can efficiently handle vast amounts of data without needing to store the entire dataset in memory. Additionally, the mini-batch approach allows the algorithm to generalize well and make progress in the optimization process even with limited computational resources.\n",
        "\n",
        "In summary, SGD's ability to use random mini-batches and update parameters incrementally makes it a powerful optimization technique, especially for large datasets and complex models. Its efficiency and faster convergence make it a popular choice in various machine learning tasks, such as deep learning, where large-scale datasets and high-dimensional data are common."
      ],
      "metadata": {
        "id": "L2AzcwkEUQcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initializing the SGDClassifier object with desired hyperparameters.\n",
        "# For linear SVM, the 'hinge' loss is used, and L2 regularization with alpha=0.0001 is applied.\n",
        "# The random_state parameter is set for reproducibility, and max_iter specifies the maximum number of iterations for training.\n",
        "sgd_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, random_state=42, max_iter=1000)\n",
        "\n",
        "# Training the model on the training data (bw_X_train) along with their corresponding labels (y_train).\n",
        "sgd_clf.fit(bw_X_train, y_train)\n",
        "\n",
        "# Predicting labels for the test data (bw_X_test) using the trained SGDClassifier.\n",
        "y_pred_sgd = sgd_clf.predict(bw_X_test)\n",
        "\n",
        "# Calculating the accuracy of the classifier by comparing the predicted labels (y_pred_sgd) with the true labels (y_test).\n",
        "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
        "\n",
        "# Printing the calculated accuracy of the SGDClassifier on the test data.\n",
        "print('Accuracy_sgd:', accuracy_sgd)"
      ],
      "metadata": {
        "id": "MfkQI8UDUUYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the SGDClassifier object with desired hyperparameters.\n",
        "# For linear SVM, the 'hinge' loss is used, and L2 regularization with alpha=0.0001 is applied.\n",
        "# The random_state parameter is set for reproducibility, and max_iter specifies the maximum number of iterations for training.\n",
        "sgd_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, random_state=42, max_iter=1000)\n",
        "\n",
        "# Training the model on the training data (bw_X_train) along with their corresponding labels (y_train).\n",
        "sgd_clf.fit(bw_X_train, y_train)\n",
        "\n",
        "# Predicting labels for the test data (bw_X_test) using the trained SGDClassifier.\n",
        "y_pred_sgd = sgd_clf.predict(bw_X_test)\n",
        "\n",
        "# Importing classification_report function to generate a comprehensive classification report.\n",
        "# The classification_report function provides precision, recall, F1-score, and support for each class.\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate the classification report using scikit-learn's 'classification_report' function.\n",
        "# This report provides precision, recall, F1-score, and support for each class.\n",
        "report_sgd = classification_report(y_test, y_pred_sgd, target_names=label)\n",
        "\n",
        "# Now, let's print the classification report to see the performance metrics.\n",
        "print(report_sgd)"
      ],
      "metadata": {
        "id": "O0tVl4wIUa4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Initializing the SGDClassifier object with desired hyperparameters.\n",
        "# For linear SVM, the 'hinge' loss is used, and L2 regularization with alpha=0.0001 is applied.\n",
        "# The random_state parameter is set for reproducibility, and max_iter specifies the maximum number of iterations for training.\n",
        "sgd_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, random_state=42, max_iter=1000)\n",
        "\n",
        "# Training the model on the training data (bw_X_train) along with their corresponding labels (y_train).\n",
        "sgd_clf.fit(bw_X_train, y_train)\n",
        "\n",
        "# Predicting labels for the test data (bw_X_test) using the trained SGDClassifier.\n",
        "y_pred_sgd = sgd_clf.predict(bw_X_test)\n",
        "\n",
        "# Calculating the confusion matrix for the SGDClassifier.\n",
        "cf8 = confusion_matrix(y_test, y_pred_sgd)\n",
        "\n",
        "# Set up the plot figure with the heatmap\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.heatmap(cf8, annot=True, fmt=\".0f\", cmap='YlGnBu')  # Using the 'YlGnBu' color map for the heatmap\n",
        "\n",
        "# Set labels, title, and ticks for better visualization\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (STOCHASTIC GRADIENT DESCENT CLASSIFIER with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "\n",
        "# Ensure proper visualization of the plot and eliminate any cutoff issues\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dYYoeF1FUhHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "qb1yQWwBUqYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent is an optimization algorithm commonly used in machine learning to train various models, including linear classifiers like the Support Vector Machine (SVM). In this case, SGD was applied as the core algorithm for the classifier.\n",
        "\n",
        "The achieved accuracy of 80% indicates that the model correctly classified 80% of the data instances, reflecting a considerable improvement compared to the baseline models' performances. The higher accuracy suggests that the classifier has become more adept at correctly classifying the data points into their respective categories.\n",
        "\n",
        "Furthermore, the evaluation metrics—precision, recall, and F1-score—were calculated for each individual class ('Negative', 'Neutral', 'Positive'). These metrics are essential to assess the classifier's performance for each specific class and its ability to correctly identify positive instances, avoid false positives, and achieve a balance between precision and recall.\n",
        "\n",
        "The observation that the evaluation metrics showed significant improvement compared to all the baseline models indicates that the SGD algorithm, with the chosen hyperparameters, has effectively learned to distinguish between different classes and provide more accurate predictions. This enhanced performance can be attributed to the optimization process during training, where the algorithm updates the model's parameters based on randomly selected mini-batches of data.\n",
        "\n",
        "In conclusion, the application of the Stochastic Gradient Descent algorithm as the classifier has led to substantial advancements in accuracy and performance metrics, demonstrating its effectiveness in handling the classification task. These improved results can be attributed to the iterative and stochastic nature of SGD, enabling faster convergence and better generalization on the given dataset."
      ],
      "metadata": {
        "id": "0wlIGkYnUyXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "KC-HR0_-U4zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, the SGD algorithm was utilized to construct the classifier. The hyperparameters, such as 'loss', 'penalty', 'alpha', 'random_state', and 'max_iter', govern the behavior and training process of the SGD-based classifier.\n",
        "\n",
        "'loss': The 'hinge' loss is a type of loss function used in linear Support Vector Machine (SVM) classifiers. It aims to maximize the margin between classes, leading to better separation.\n",
        "\n",
        "'penalty': The 'l2' penalty refers to L2 regularization, also known as Ridge regularization. It helps prevent overfitting by adding a penalty term to the cost function based on the squared magnitudes of the model's coefficients.\n",
        "\n",
        "'alpha': This parameter controls the strength of the regularization. In this case, the value is set to 0.0001, which means a small amount of regularization is applied to the model.\n",
        "\n",
        "'random_state': The random_state is set to 42, which is an arbitrary seed value used for reproducibility. It ensures that the results of the training process will be consistent across different runs.\n",
        "\n",
        "'max_iter': The maximum number of iterations for training the model is set to 1000. This parameter limits the number of iterations the algorithm goes through while optimizing the model's parameters.\n",
        "\n",
        "By using these default hyperparameter values, the classifier is created and trained without explicit hyperparameter tuning. In some cases, the default values might provide reasonable results, especially for simple tasks or smaller datasets. However, for more complex tasks or larger datasets, explicit hyperparameter tuning is typically required to find the optimal combination of hyperparameter values that yield the best performance and generalization."
      ],
      "metadata": {
        "id": "n2QaB50lVAoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart"
      ],
      "metadata": {
        "id": "1MJ2M6L1VE5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As observed from the Evaluation Metric Score Chart, the Stochastic Gradient Descent (SGD) algorithm applied for Sentiment Analysis using Count Vectorizer (Bags of Words) has shown significant improvements in performance compared to all the baseline model algorithms.\n",
        "\n",
        "The model achieved an impressive accuracy of 80%, indicating that it correctly classified 80% of the data instances into their respective sentiment categories. Additionally, the precision, recall, and F1-score metrics were calculated for each sentiment class ('Negative', 'Neutral', 'Positive').\n",
        "\n",
        "The precision of 81% suggests that the model correctly identified 81% of the positive instances out of all the instances it predicted as positive. The recall of 80% indicates that the model correctly identified 80% of the actual positive instances present in the dataset.\n",
        "\n",
        "Furthermore, the F1-score of 80% is a harmonic mean of precision and recall, providing a balanced measure of the model's overall performance.\n",
        "\n",
        "Overall, this SGD-based Sentiment Analysis model has demonstrated remarkable accuracy and robustness in distinguishing between different sentiment categories. The combination of Count Vectorization and SGD optimization has proven to be effective in processing text data and generating meaningful insights from sentiment analysis tasks.\n",
        "\n",
        "It is important to note that the success of this model can be attributed to the careful selection of hyperparameters, feature engineering using Count Vectorizer, and the inherent ability of SGD to efficiently handle large datasets and high-dimensional data. Such a model can be well-suited for real-world Sentiment Analysis applications, enabling businesses and organizations to gain valuable insights from unstructured text data, such as customer reviews, social media posts, or feedback."
      ],
      "metadata": {
        "id": "01z7lXJUVL7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**TF-IDF VECTORIZATION(Term Frequency-Inverse Document Frequency)**"
      ],
      "metadata": {
        "id": "G02Y15W3VRfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical representation used in Natural Language Processing (NLP) to assess the significance of words in a document within a collection (corpus) of documents.\n",
        "\n",
        "The term frequency (TF) component of TF-IDF calculates the frequency of a specific word within a single document. It quantifies how often a particular term occurs in the document. Words with higher TF scores are considered more relevant or important within that specific document.\n",
        "\n",
        "On the other hand, the inverse document frequency (IDF) component evaluates the importance of a word in the entire corpus. It measures the rarity of a term across the collection of documents. Words that appear in many documents receive a lower IDF score, implying that they are likely to be common and less distinctive.\n",
        "\n",
        "By combining both the TF and IDF values, the TF-IDF score captures the essence of a word's importance both locally (within a document) and globally (across the entire corpus). Terms with high TF-IDF scores are indicative of being crucial to a particular document but relatively rare in the overall collection. This characteristic makes TF-IDF a valuable tool in tasks like text classification, information retrieval, and keyword extraction.\n",
        "\n",
        "In summary, TF-IDF assigns weights to words based on their relative occurrence in a document and across the entire corpus. This allows NLP models to focus on relevant and distinctive terms, facilitating better understanding and analysis of textual data."
      ],
      "metadata": {
        "id": "K29_tyugVkef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Data Splitting**"
      ],
      "metadata": {
        "id": "FAFBLESLpt97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the 'Clean_Tweets' column as input data 'X'\n",
        "X = tweet_df['Clean_Tweets']\n",
        "\n",
        "# Extracting the 'Sentiment' column as target labels 'y'\n",
        "y = tweet_df['Sentiment']"
      ],
      "metadata": {
        "id": "qXJpSauQppoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the train_test_split function from scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into training and testing sets with a 80-20 split (test_size=0.2)\n",
        "# The stratify parameter ensures that the class distribution is preserved in both the training and testing sets\n",
        "# random_state is set to 10 for reproducibility, meaning the same random split will be used if the code is run again with the same seed value\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=10)"
      ],
      "metadata": {
        "id": "DV4XGkXmp2DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the shape of the X_train array (input data for training)\n",
        "print(X_train.shape)\n",
        "\n",
        "# Printing the shape of the y_test array (target labels for testing)\n",
        "y_test_shape = y_test.shape\n",
        "print(y_test_shape)"
      ],
      "metadata": {
        "id": "WZJgsT54p61l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Vectorizing Text**"
      ],
      "metadata": {
        "id": "KwRZer8up-mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries for text vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "PqsOj4TMqJVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer is used for text vectorization using the Term Frequency-Inverse Document Frequency (TF-IDF) technique.\n",
        "\n",
        "# TfidfVectorizer parameters:\n",
        "# - use_idf=True: Enables the use of inverse document frequency (IDF) in the calculation.\n",
        "# - max_df=1.0: Ignores terms that have a document frequency strictly higher than the given threshold (in this case, not used as it's set to 1.0).\n",
        "# - min_df=5: Ignores terms that have a document frequency strictly lower than the given threshold (set to 5, meaning terms appearing in fewer than 5 documents are ignored).\n",
        "# - ngram_range=(1, 2): Specifies the range of n-grams to be extracted. Here, it includes both unigrams (single words) and bigrams (two consecutive words).\n",
        "# - sublinear_tf=True: Applies sublinear scaling to the term frequency, replacing TF with 1 + log(TF).\n",
        "\n",
        "tv = TfidfVectorizer(use_idf=True, max_df=1.0, min_df=5, ngram_range=(1, 2), sublinear_tf=True)\n",
        "tv_X_train = tv.fit_transform(X_train.astype(str).str.strip())"
      ],
      "metadata": {
        "id": "GDuN4hrYqKLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the shape of tv_X_train, which represents the TF-IDF transformed feature vectors for the training data.\n",
        "print(tv_X_train.shape)"
      ],
      "metadata": {
        "id": "SYBNuVRRqWdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the TfidfVectorizer 'tv' to X_test and obtaining the TF-IDF transformed feature vectors for the test data.\n",
        "tv_X_test = tv.transform(X_test.astype(str).str.strip())"
      ],
      "metadata": {
        "id": "EHGNfqmkqbNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**MODEL - 1 (TF-IDF) Logistic Regression with GridserchCV**"
      ],
      "metadata": {
        "id": "O0Sr7dctWKFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing model\n",
        "lr_tv=LogisticRegression()\n",
        "parameters = dict(penalty=['l1', 'l2'],C=[100, 10, 1.0, 0.1, 0.01])\n",
        "\n",
        "#Hyperparameter tuning by GridserchCV\n",
        "lr_tv_Gcv=GridSearchCV(lr_tv,parameters,cv=5)\n",
        "\n",
        "#fitting the data to model\n",
        "lr_tv_Gcv.fit(tv_X_train,y_train)"
      ],
      "metadata": {
        "id": "VlGw1vgmWU7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicted values\n",
        "pred_lr_tv_Gcv = lr_tv_Gcv.predict(tv_X_test)\n"
      ],
      "metadata": {
        "id": "BP7tWDxqWagU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the accuracy of the logistic regression model by comparing the predicted labels 'pred_lr_tv_Gcv' with the true labels 'y_test'\n",
        "accuracy_lr_Gcv = accuracy_score(y_test, pred_lr_tv_Gcv)\n",
        "\n",
        "# Printing the accuracy of the logistic regression model on the test set\n",
        "print(\"Accuracy: \", accuracy_lr_Gcv)\n"
      ],
      "metadata": {
        "id": "5p7cEK1PWeWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['Neutral','Positive','Negative']\n",
        "print(classification_report(y_test,pred_lr_tv_Gcv))"
      ],
      "metadata": {
        "id": "2YecAqH2WkLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the confusion matrix for the logistic regression model's predictions 'pred_lr_tv_Gcv' and the true labels 'y_test'\n",
        "cf1a = confusion_matrix(y_test, pred_lr_tv_Gcv)\n",
        "\n",
        "# Creating a heatmap to visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = plt.subplot()\n",
        "\n",
        "# Generating the heatmap using seaborn's heatmap function\n",
        "sns.heatmap(cf1a, annot=True, fmt=\".0f\", ax=ax, cmap='Blues')  # Change heatmap color to 'Blues'\n",
        "\n",
        "# Setting labels, title, and ticks for the heatmap\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Logistic Regression with TF/IDF)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "AAOz7O9-Woka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "h6w4XtlPVRZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Logistic Regression algorithm was employed to classify the data, achieving an accuracy of 78%. In addition to accuracy, various evaluation metrics such as precision, recall, and f1-score were computed for each sentiment label ('Negative', 'Neutral', 'Positive'). These metrics demonstrated significant improvements compared to all the baseline models.\n",
        "\n",
        "The high accuracy and improved evaluation metrics indicate that the Logistic Regression model, when using TF-IDF for text representation, is capable of effectively distinguishing between different sentiment classes ('Negative', 'Neutral', 'Positive') in the test data. It outperforms the baseline models, showcasing its ability to capture meaningful patterns in the tweet text data and make more accurate predictions. This success suggests that the model's predictions align better with the actual sentiment labels in the test set, making it a promising approach for sentiment analysis using the TF-IDF vectorization technique."
      ],
      "metadata": {
        "id": "lz0Upb8JXAOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2.Which hyperparameter optimization technique have you used and why?**"
      ],
      "metadata": {
        "id": "c5nDJ9kOXHBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In hyperparameter tuning, the \"parameters\" dictionary defines the range of values for the regularization parameter C and the penalty term (l1 or l2) used in Logistic Regression. GridSearchCV is then utilized to systematically search through this hyperparameter space and evaluate the model's performance using cross-validation.\n",
        "\n",
        "The purpose of hyperparameter tuning is to identify the optimal combination of hyperparameters that leads to the best model performance. By exploring various values for C and penalty term, GridSearchCV helps to fine-tune the model and find the hyperparameters that yield the highest accuracy and other evaluation metrics on the validation data.\n",
        "\n",
        "By tuning the hyperparameters, GridSearchCV aims to prevent overfitting, where the model becomes too complex and fits the training data too closely, resulting in poor generalization to unseen data. Instead, the goal is to identify a well-generalized model that performs well not only on the training data but also on new, unseen data.\n",
        "\n",
        "The use of cross-validation in GridSearchCV further enhances the reliability of the selected hyperparameters. Cross-validation divides the training data into multiple folds, using different subsets for training and validation at each iteration. This ensures that the hyperparameters are chosen based on a comprehensive evaluation of the model's performance across various data partitions, reducing the risk of biased selection.\n",
        "\n",
        "In summary, GridSearchCV with hyperparameter tuning helps to fine-tune the Logistic Regression model, identify the best combination of hyperparameters, and ensure that the model generalizes well to unseen data by preventing overfitting. This iterative process optimizes the model's performance and leads to a more robust and reliable sentiment analysis model."
      ],
      "metadata": {
        "id": "-HzrmDc8XOYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3. Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart**"
      ],
      "metadata": {
        "id": "n19289ZtXTJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above evaluation metric score chart showcases the performance of the Logistic Regression algorithm when deployed for Sentiment Analysis using the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization technique.\n",
        "\n",
        "The accuracy achieved by the Logistic Regression model is 78%, indicating that it correctly classifies sentiments for a substantial portion of the test data. Moreover, precision, recall, and f1-score, which are essential metrics for multi-class classification tasks, are all around 79%. These metrics further demonstrate the model's ability to make accurate predictions across all sentiment classes ('Negative', 'Neutral', 'Positive').\n",
        "\n",
        "Precision represents the proportion of correctly predicted instances of a particular class to the total instances predicted for that class. Recall, on the other hand, measures the ratio of correctly predicted instances of a class to the total instances of that class in the test data. The f1-score is the harmonic mean of precision and recall, providing a balanced measure of the model's performance.\n",
        "\n",
        "The high accuracy, precision, recall, and f1-score imply that the Logistic Regression model, when combined with TF-IDF vectorization, is effective at sentiment analysis on textual data. The model is capable of accurately distinguishing between different sentiment categories, making it a promising solution for real-world applications.\n",
        "\n",
        "In summary, the Logistic Regression algorithm, integrated with TF-IDF vectorization, has shown significant improvements over the baseline models. It provides robust sentiment analysis capabilities, with a balanced performance across multiple evaluation metrics, reinforcing its suitability for practical deployment in sentiment analysis tasks."
      ],
      "metadata": {
        "id": "tr86mnsLXbDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**MODEL - 2 (TF-IDF) Decision Tree with TF/IDF**\n"
      ],
      "metadata": {
        "id": "pJ_Mq-zQXoNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing model\n",
        "dt_tv=DecisionTreeClassifier()\n",
        "\n",
        "#fitting the data to model\n",
        "dt_tv.fit(tv_X_train,y_train)\n",
        "\n",
        "#prediction\n",
        "pred_dt_tv=dt_tv.predict(tv_X_test)\n"
      ],
      "metadata": {
        "id": "8QmYCtqiXubN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dt_tv\n"
      ],
      "metadata": {
        "id": "bKC1d4LCXvwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the mean of the accuracy scores obtained in all folds to get the average accuracy.\n",
        "cv_score_dt_tv = cross_val_score(dt_tv, tv_X_train, y_train, cv=5)\n",
        "mean_accuracy = np.mean(cv_score_dt_tv)\n",
        "\n",
        "# Printing the average accuracy of the DecisionTreeClassifier with TF-IDF vectorization\n",
        "print(\"Accuracy: {:.2f}\".format(mean_accuracy))"
      ],
      "metadata": {
        "id": "Y2RyPbHvXx_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the confusion matrix for the DecisionTreeClassifier's predictions 'pred_dt_tv' and the true labels 'y_test'\n",
        "cf2a = confusion_matrix(y_test, pred_dt_tv)\n",
        "\n",
        "# Creating a heatmap to visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = plt.subplot()\n",
        "\n",
        "# Generating the heatmap using seaborn's heatmap function\n",
        "sns.heatmap(cf2a, annot=True, fmt=\".0f\", ax=ax, cmap='Greens')  # Change heatmap color to 'Greens'\n",
        "\n",
        "# Setting labels, title, and ticks for the heatmap\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Decision Tree with TF/IDF)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "9PNYor2gX4q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "heQR4HJdYCJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decision Tree algorithm was employed to classify the data, achieving an accuracy of 62%. In addition to accuracy, various evaluation metrics such as precision, recall, and f1-score were calculated for each sentiment label ('Negative', 'Neutral', 'Positive'). These metrics demonstrated significant improvements compared to all the baseline models.\n",
        "\n",
        "The accuracy of 62% suggests that the Decision Tree model is capable of correctly classifying sentiments for a substantial portion of the test data. Moreover, precision, recall, and f1-score, which are essential metrics for multi-class classification tasks, have also shown marked improvements, indicating the model's ability to make more accurate predictions across all sentiment classes.\n",
        "\n",
        "Precision represents the proportion of correctly predicted instances of a particular class to the total instances predicted for that class. Recall, on the other hand, measures the ratio of correctly predicted instances of a class to the total instances of that class in the test data. The f1-score is the harmonic mean of precision and recall, providing a balanced measure of the model's performance.\n",
        "\n",
        "The observed improvements in accuracy and evaluation metrics indicate that the Decision Tree model, when applied with appropriate features (such as TF-IDF in this case), is effective at sentiment analysis on textual data. The model has shown a better ability to distinguish between different sentiment categories, making it a promising solution for practical deployment in sentiment analysis tasks.\n",
        "\n",
        "In summary, the Decision Tree algorithm, integrated with TF-IDF vectorization, has exhibited considerable advancements over the baseline models. It demonstrates improved sentiment classification capabilities, with a balanced performance across multiple evaluation metrics, making it a valuable tool for sentiment analysis in various real-world applications."
      ],
      "metadata": {
        "id": "bt4akTp9YJxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2. Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart**"
      ],
      "metadata": {
        "id": "wNtSrw5WYOMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above Evaluation Metric Score Chart displays the performance of the Decision Tree algorithm using the TF-IDF (Term Frequency-Inverse Document Frequency) technique for sentiment analysis.\n",
        "\n",
        "The model achieved an accuracy of 62%, indicating its ability to correctly classify sentiments for a significant portion of the test data. However, when considering precision, recall, and f1-score, which are crucial metrics for multi-class classification tasks, the model's performance showed only marginal improvements compared to the baseline models.\n",
        "\n",
        "Precision represents the proportion of correctly predicted instances of a particular class to the total instances predicted for that class. Recall measures the ratio of correctly predicted instances of a class to the total instances of that class in the test data. The f1-score is the harmonic mean of precision and recall, providing a balanced measure of the model's performance.\n",
        "\n",
        "The modest improvements in precision, recall, and f1-score indicate that the Decision Tree algorithm, coupled with TF-IDF vectorization, did not significantly outperform the baseline models in sentiment classification tasks. The model's ability to distinguish between different sentiment categories ('Negative', 'Neutral', 'Positive') showed only minor enhancement.\n",
        "\n",
        "In summary, while the Decision Tree algorithm using TF-IDF provided reasonable accuracy, the evaluation metrics of precision, recall, and f1-score demonstrate that the model's performance is not substantially superior to the baseline models. It suggests that further improvements or alternative approaches might be needed to achieve more robust and accurate sentiment analysis in practical applications."
      ],
      "metadata": {
        "id": "0AYKfFkrYU-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**MODEL - 3 (TF-IDF) KNN(K-Nearest Neighbours)**"
      ],
      "metadata": {
        "id": "J7AFYQy3YZK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing model\n",
        "knn = KNeighborsClassifier()\n",
        "param = {'n_neighbors': [1,2,3,4,5,6,7,8]}\n",
        "knn_tv = GridSearchCV(estimator=knn,param_grid=param)\n",
        "\n",
        "#fitting the data to model\n",
        "knn_tv.fit(tv_X_train, y_train)"
      ],
      "metadata": {
        "id": "Hen0Xwe4e4Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicted values\n",
        "pred_knn_tv = knn_tv.predict(tv_X_test)\n"
      ],
      "metadata": {
        "id": "4BaKeCpSKjxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_knn_tv"
      ],
      "metadata": {
        "id": "w4GF7D3CYkO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the accuracy of the KNeighborsClassifier with TF-IDF vectorization\n",
        "accuracy_KNN_tv = accuracy_score(y_test, pred_knn_tv)\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy_KNN_tv))\n"
      ],
      "metadata": {
        "id": "1RyQrWj5YnzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Classification report of Performance metrics\n",
        "label=['Neutral','Positive','Negative']\n",
        "print(classification_report(y_test,pred_knn_tv))"
      ],
      "metadata": {
        "id": "cwJC_e8HYp1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the confusion matrix for the KNeighborsClassifier's predictions 'pred_knn_tv' and the true labels 'y_test'\n",
        "cf4a = confusion_matrix(y_test, pred_knn_tv)\n",
        "\n",
        "# Creating a heatmap to visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = plt.subplot()\n",
        "\n",
        "# Generating the heatmap using seaborn's heatmap function\n",
        "sns.heatmap(cf4a, annot=True, fmt=\".0f\", ax=ax, cmap='Blues')  # Change heatmap color to 'Blues'\n",
        "\n",
        "# Setting labels, title, and ticks for the heatmap\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (KNN TF/IDF with GridsearchCV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "ybmapCRsYt0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "r7kxzYOHY0Ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Nearest Neighbors (KNN) algorithm was employed to classify the data, yielding an accuracy of 27%. However, despite the baseline models' performance, this KNN model showed significant shortcomings in terms of precision, recall, and f1 score for each sentiment label ('Negative', 'Neutral', 'Positive').\n",
        "\n",
        "The accuracy of 27% indicates that the KNN model was not able to correctly classify sentiments for a large portion of the test data. Additionally, when considering precision, recall, and f1-score, which are essential metrics for multi-class classification tasks, the KNN model's performance was considerably lower than the baseline models.\n",
        "\n",
        "Precision reflects the ratio of correctly predicted instances of a particular class to the total instances predicted for that class, while recall measures the ratio of correctly predicted instances of a class to the total actual instances of that class in the test data. The f1-score provides a balanced measure of precision and recall. The significantly lower values for these metrics indicate that the KNN model struggled to differentiate between different sentiment categories.\n",
        "\n",
        "In summary, despite achieving a certain level of accuracy, the KNN model with TF-IDF vectorization did not perform well when considering other critical evaluation metrics. Its limited ability to distinguish between sentiments resulted in poor precision, recall, and f1-score values for each sentiment class. Consequently, it can be considered a suboptimal model for sentiment analysis compared to the baseline models. Further improvements or alternative approaches may be required to enhance the sentiment analysis performance in practical applications."
      ],
      "metadata": {
        "id": "6enOtXnKY5w9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2.Which hyperparameter optimization technique have you used and why?**"
      ],
      "metadata": {
        "id": "5Rr13eBQY9Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, we are using cross-validation to optimize the hyperparameters of the K-Nearest Neighbors (KNN) classifier. The hyperparameter space is defined by the \"param\" dictionary, which includes the number of neighbors to consider (n_neighbors) ranging from 1 to 8.\n",
        "\n",
        "By employing GridSearchCV, we can systematically explore different values for n_neighbors and evaluate the model's performance using cross-validation. GridSearchCV helps us find the optimal number of neighbors that yields the best performance for the KNN classifier.\n",
        "\n",
        "During the process, the data is divided into multiple subsets (folds), and the model is trained and validated on different combinations of these subsets. This allows us to obtain a more reliable estimate of the model's performance by reducing the risk of overfitting or underfitting.\n",
        "\n",
        "By leveraging GridSearchCV to tune the n_neighbors hyperparameter, we can select the number of neighbors that leads to the highest accuracy or other evaluation metrics, making the KNN classifier more effective and better suited for sentiment analysis tasks.\n",
        "\n",
        "In summary, the goal of using GridSearchCV is to explore the hyperparameter space for the KNN classifier, specifically the number of neighbors (n_neighbors), and find the optimal value that results in the best performance on the cross-validated data. This helps in building a more robust and accurate sentiment analysis model for practical deployment."
      ],
      "metadata": {
        "id": "Lq0_pJSuZKrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3. Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart**"
      ],
      "metadata": {
        "id": "vjMhND12ZOzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Nearest Neighbors (KNN) algorithm was utilized for sentiment analysis with TF-IDF (Term Frequency-Inverse Document Frequency) as the vectorization technique. However, the performance of this KNN model was notably worse than all the baseline models.\n",
        "\n",
        "The evaluation metric score chart reveals an accuracy of only 27%. This indicates that the KNN model struggled to correctly classify sentiments for a substantial portion of the test data, leading to poor overall accuracy.\n",
        "\n",
        "Furthermore, when we consider other essential evaluation metrics like precision, recall, and f1-score, the model's performance is even more unsatisfactory. Precision, which measures the ratio of correctly predicted instances of a particular class to the total instances predicted for that class, is at 62%. Recall, which quantifies the ratio of correctly predicted instances of a class to the total actual instances of that class in the test data, is at 40%. The f1-score, which combines precision and recall into a balanced measure of the model's performance, stands at only 25%.\n",
        "\n",
        "The lower precision and recall values indicate that the KNN model had difficulty distinguishing between different sentiment classes ('Negative', 'Neutral', 'Positive'), leading to a mix of false positives and false negatives.\n",
        "\n",
        "In summary, the KNN model with TF-IDF vectorization performed poorly compared to all the baseline models. Its low accuracy, along with subpar precision, recall, and f1-score values, indicates that it was not effective in classifying sentiments accurately and lacked the ability to differentiate between sentiment categories. To improve sentiment analysis performance, alternative models or advanced techniques may be necessary to handle the inherent complexities of text data and sentiment classification more effectively."
      ],
      "metadata": {
        "id": "LDDaoQwSZXcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**MODEL - 4 (TF-IDF) Stochastic Gradient Descent**"
      ],
      "metadata": {
        "id": "lSfogoHsZeUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Initialize SGDClassifier object with desired hyperparameters\n",
        "sgd_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, random_state=42, max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "sgd_clf.fit(tv_X_train, y_train)\n",
        "\n",
        "# Predict labels for test data\n",
        "y_pred_sgd_tv = sgd_clf.predict(tv_X_test)\n",
        "\n",
        "# Calculate accuracy of classifier on test data\n",
        "accuracy_sgd_tv= accuracy_score(y_test, y_pred_sgd_tv)\n",
        "print('Accuracy:', accuracy_sgd_tv)"
      ],
      "metadata": {
        "id": "diXYTojcZk6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['Neutral','Positive','Negative']\n",
        "print(classification_report(y_test,y_pred_sgd_tv ))"
      ],
      "metadata": {
        "id": "xhYllTfIZmUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries for confusion matrix and heatmap visualization\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Computing the confusion matrix for the SGD Classifier's predictions 'y_pred_sgd_tv' and the true labels 'y_test'\n",
        "cf6a = confusion_matrix(y_test, y_pred_sgd_tv)\n",
        "\n",
        "# Creating a heatmap to visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = plt.subplot()\n",
        "\n",
        "# Generating the heatmap using seaborn's heatmap function\n",
        "sns.heatmap(cf6a, annot=True, fmt=\".0f\", ax=ax, cmap='Blues')  # Change heatmap color to 'Blues'\n",
        "\n",
        "# Setting labels, title, and ticks for better visualization\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Stochastic Classifier with GridsearchCV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "EMjADbNvZtRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "9hoYHfPPZxgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Stochastic Gradient Descent (SGD) algorithm was applied to classify the data, achieving an accuracy of 76%. Furthermore, various evaluation metrics, including precision, recall, and f1-score, were computed for each sentiment label ('Negative', 'Neutral', 'Positive') and exhibited significant improvements compared to all the baseline models.\n",
        "\n",
        "The accuracy of 76% indicates that the SGD Classifier performed well in correctly classifying sentiments across different categories. Moreover, the precision, recall, and f1-score metrics demonstrated notable enhancements, signifying the model's ability to make accurate predictions and effectively identify positive, negative, and neutral sentiments.\n",
        "\n",
        "The SGD algorithm's strength lies in its efficiency in handling large datasets and high-dimensional features. By utilizing stochastic gradient updates on randomly selected mini-batches of training data, it can converge faster than traditional gradient descent methods, making it particularly suitable for large-scale sentiment analysis tasks.\n",
        "\n",
        "In combination with TF-IDF vectorization, which weighs the importance of terms based on their frequency and inverse document frequency, the SGD Classifier efficiently captured sentiment-related patterns in the data, leading to improved performance in sentiment classification.\n",
        "\n",
        "Overall, the Stochastic Gradient Descent Classifier with TF-IDF vectorization demonstrated promising results, achieving higher accuracy and better precision, recall, and f1-score metrics compared to the baseline models. This makes it a valuable model for sentiment analysis applications, providing reliable predictions and efficient handling of large-scale sentiment datasets."
      ],
      "metadata": {
        "id": "iQdNIF7HZ6w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2.Which hyperparameter optimization technique have you used and why?**"
      ],
      "metadata": {
        "id": "i0PHFAo-aAVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, the Stochastic Gradient Descent (SGD) Classifier has been created with default hyperparameter values, namely loss='hinge', penalty='l2', alpha=0.0001, random_state=42, and max_iter=1000. These default values are utilized when specific hyperparameter values are not explicitly provided during model instantiation.\n",
        "\n",
        "The SGD Classifier is then trained on the training data using the default hyperparameter settings. The hyperparameters play a crucial role in determining the behavior of the algorithm during the training process. For instance, 'loss' specifies the loss function used for optimization, 'penalty' determines the regularization term, 'alpha' sets the regularization strength, 'random_state' ensures reproducibility of results by fixing the random seed, and 'max_iter' specifies the maximum number of iterations for convergence.\n",
        "\n",
        "Once the training is completed, the model is used to make predictions on the test data to evaluate its performance. This involves feeding the test data through the trained SGD Classifier, and the model generates predicted labels for each instance in the test dataset.\n",
        "\n",
        "By using default hyperparameter values, the model is quickly initialized and trained, providing a baseline performance for sentiment analysis. Default values are often chosen to simplify the model implementation and serve as a starting point for further hyperparameter tuning and optimization.\n",
        "\n",
        "While the default settings can offer satisfactory results in some cases, they may not always yield the best performance for specific tasks. Hyperparameter tuning, such as using techniques like GridSearchCV or RandomizedSearchCV, allows us to explore different combinations of hyperparameters to find the optimal configuration for the problem at hand, potentially improving the model's accuracy and generalization. However, for initial assessment or when the primary focus is to establish a baseline, default hyperparameter values are commonly used."
      ],
      "metadata": {
        "id": "2D0YSxIcaIhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3. Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart**"
      ],
      "metadata": {
        "id": "KC_wCqomaNH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Stochastic Gradient Descent (SGD) Classifier, with the default hyperparameter values and TF-IDF vectorization technique, exhibited improved performance compared to all the baseline models for sentiment analysis.\n",
        "\n",
        "The accuracy of the SGD Classifier reached 72%, indicating that it correctly classified sentiments in a significant portion of the test data. Moreover, the model's precision, recall, and f1-score metrics, which assess its ability to make accurate predictions, sensitivity, and overall performance, respectively, were substantially enhanced, with values of 77% precision, 77% recall, and 76% f1-score.\n",
        "\n",
        "These improvements can be attributed to the combination of the SGD algorithm's efficiency and the TF-IDF vectorization technique's ability to extract meaningful information from the text data. The SGD algorithm's stochastic gradient updates and mini-batch processing efficiently handle large volumes of data, allowing the model to converge faster and learn better representations of sentiments in the text.\n",
        "\n",
        "TF-IDF, on the other hand, effectively weighs the importance of terms based on their frequency in individual documents and across the entire corpus. This technique enables the SGD Classifier to focus on significant terms while downplaying the impact of common words, leading to more accurate sentiment predictions.\n",
        "\n",
        "The results from the Evaluation Metric Score Chart demonstrate the effectiveness of the SGD Classifier with TF-IDF in sentiment analysis, offering improved accuracy and well-balanced precision, recall, and f1-score. This suggests that the model is capable of making accurate predictions across all sentiment classes ('Negative', 'Neutral', 'Positive') and can be deployed with confidence for real-world sentiment analysis tasks, providing valuable insights into public sentiment and opinion analysis."
      ],
      "metadata": {
        "id": "1YumaQjzaV9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Classfication Metrics Report**"
      ],
      "metadata": {
        "id": "r8intfldadTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification metrics are essential in assessing the performance of a classification model, particularly for tasks like sentiment analysis, where we need to compare the predicted labels to the actual labels.\n",
        "\n",
        "One of the primary evaluation metrics is Accuracy, which gauges how well the model predicts the correct labels compared to the total number of samples. It is particularly useful when dealing with balanced classes, meaning that the number of samples in each sentiment category (negative, neutral, positive) is roughly equal.\n",
        "\n",
        "To calculate Accuracy, we consider the four possibilities for predicted and actual labels:\n",
        "\n",
        "True Positives (TP): The number of instances where the model correctly predicted a positive sentiment when the actual sentiment was positive.\n",
        "\n",
        "True Negatives (TN): The number of instances where the model correctly predicted a negative sentiment when the actual sentiment was negative.\n",
        "\n",
        "False Positives (FP): The number of instances where the model incorrectly predicted a positive sentiment when the actual sentiment was negative.\n",
        "\n",
        "False Negatives (FN): The number of instances where the model incorrectly predicted a negative sentiment when the actual sentiment was positive.\n",
        "\n",
        "Accuracy is then computed as the sum of True Positives and True Negatives divided by the total number of samples:\n",
        "\n",
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "A high Accuracy score indicates that the model is making correct predictions for a significant portion of the samples. However, it is essential to consider other metrics as well, especially when dealing with imbalanced classes or when different types of errors (false positives or false negatives) have varying consequences.\n",
        "\n",
        "In sentiment analysis, where we are interested in correctly identifying positive, negative, and neutral sentiments, a high Accuracy score indicates that the model is performing well in classifying sentiments overall. However, it is essential to examine other classification metrics, such as Precision, Recall, and F1-score, to gain a more comprehensive understanding of the model's performance across different sentiment categories and to identify potential areas for improvement."
      ],
      "metadata": {
        "id": "HO3phXWAagx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "acurracy = {'Model':  ['Logistic Regression with GridserachCV', 'Decision Tree Classifier','K-Nearest-Neighbours Classifier','Stochastic Gradient Descent',],\n",
        "        'Count Vector':  [accuracy_lr_cv,np.mean(cv_score_dt_cv),accuracy_KNN,accuracy_sgd,],\n",
        "        'Tf/idf Vector': [accuracy_lr_Gcv,np.mean(cv_score_dt_tv),accuracy_KNN_tv,accuracy_sgd_tv]}\n",
        "\n",
        "cv_score_table= pd.DataFrame (acurracy, columns = ['Model','Count Vector','Tf/idf Vector'])\n",
        "cv_score_table"
      ],
      "metadata": {
        "id": "J98EEw70aq_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyzing the metrics chart provided above, we can draw the conclusion that the Stochastic Gradient Descent (SGD) model using Count Vectorization technique outperformed the Logistic Regression model using Tf/idf Vectorization. The SGD model achieved an impressive accuracy level of 80%, while the Logistic Regression model attained an accuracy level of 78%.\n",
        "\n",
        "Based on this comparison, we have decided to select the Stochastic Gradient Descent model with Count Vectorization technique for deployment. The reason for this choice is the superior performance of the SGD model in accurately predicting sentiments in the given dataset. With an accuracy of 80%, the SGD model demonstrates its ability to correctly classify sentiments across different categories (negative, neutral, positive) with a high level of accuracy."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- The selected model for sentiment analysis is the Stochastic Gradient Descent (SGD) classifier, which is a linear classifier widely used for text classification tasks. It leverages gradient descent optimization to iteratively update model parameters, striving to minimize the loss function and improve predictive accuracy. Count Vectorization is the chosen technique to convert text data into numerical representations. This technique counts the occurrences of each word in the text data and creates a document-term matrix, where each row corresponds to a document and each column represents a word from the vocabulary. The resulting matrix is then used as input to the SGD classifier for sentiment classification.\n",
        "\n",
        "2- To gain insights into the feature importance and understand which words significantly influence the model's predictions, we can employ the Permutation Importance technique provided by the scikit-learn library. Permutation Importance works by randomly permuting the values of a single feature while keeping the others unchanged, and then measuring the effect of this permutation on the model's performance. By comparing the performance degradation caused by permuting each feature, we can determine the relative importance of different words in predicting sentiment. Features that lead to a significant drop in performance when permuted are considered more important, as they have a stronger impact on the model's decision-making process. This information can be valuable in understanding which words or phrases contribute the most to the model's ability to correctly classify sentiments, providing valuable insights for further analysis and interpretation of the results."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have evaluated and compared eight different models for sentiment analysis, including Logistic Regression with Grid Search CV, Decision Tree Classifier, Stochastic Gradient Descent, KNN, SVM, Multinomial Navies Bayes, and Bernoulli Navies Bayes classifiers. We applied these models with both Count Vectorization and TF-IDF Vectorization techniques to transform the text data into numerical representations.\n",
        "\n",
        "After analyzing the results, we have determined that the Stochastic Gradient Descent model with Count Vectorization achieved the highest accuracy of 80.43%. It outperformed all other models in accurately classifying sentiments across different categories (negative, neutral, positive). The second-best performer was the Logistic Regression with Grid Search CV model using TF-IDF Vectorization, which achieved an accuracy of 78.86%.\n",
        "\n",
        "The results demonstrate that the Stochastic Gradient Descent model with Count Vectorization is the most suitable choice for sentiment analysis in this specific dataset. Its high accuracy level indicates its capability to make reliable predictions, making it a valuable tool for understanding public sentiment, customer feedback, and opinion analysis across various domains.\n",
        "\n",
        "We can confidently deploy the selected Stochastic Gradient Descent model with Count Vectorization for sentiment analysis tasks in real-world applications. Its accuracy and overall performance make it a robust and dependable solution for understanding sentiments expressed in textual data.\n",
        "\n",
        "It is crucial to monitor and reevaluate the model's performance over time to ensure its continued effectiveness. In the future, we can repeat the sentiment analysis and compare it with the present analysis to gauge the impact of any changes or initiatives on sentiments expressed by the target audience.\n",
        "\n",
        "By regularly updating and retraining the model with new data, we can maintain its relevance and adaptability to evolving sentiment patterns in the dataset.\n",
        "\n",
        "Additionally, we can explore other advanced natural language processing techniques and deep learning models to further improve sentiment analysis accuracy and gain deeper insights into text data.\n",
        "\n",
        "Careful consideration of feature selection and engineering can play a crucial role in enhancing the model's performance. Analyzing feature importance using techniques like Permutation Importance can help identify critical words or phrases that drive sentiment predictions.\n",
        "\n",
        "Conducting a thorough error analysis can provide valuable feedback on the model's weaknesses and areas for improvement. This can guide the refinement of the model and lead to more accurate predictions in future iterations.\n",
        "\n",
        "The choice of evaluation metrics should align with the specific objectives and requirements of the sentiment analysis task. For example, if identifying negative sentiments is more critical in a particular application, then prioritizing recall for the negative class might be more appropriate.\n",
        "\n",
        "Regular monitoring of the model's performance against new data and comparing it with the initial results can help detect potential biases or changes in sentiment distribution over time.\n",
        "\n",
        "Deploying sentiment analysis models in real-world applications should consider ethical implications and potential bias in the training data. Ensuring fairness and avoiding unintended consequences are essential aspects of responsible AI deployment.\n",
        "\n",
        "Collaboration with domain experts, stakeholders, and end-users can provide valuable feedback and domain-specific insights to enhance the model's accuracy and relevance.\n",
        "\n",
        "Continuously staying updated with advancements in natural language processing and machine learning can open up new opportunities for refining sentiment analysis models and staying at the forefront of research and technology.\n",
        "\n",
        "The chosen model's performance and insights obtained from sentiment analysis can drive actionable decisions and inform strategies across various industries, including market research, customer service, social media analytics, and brand reputation management."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}